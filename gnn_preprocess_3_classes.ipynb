{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bfdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix,\n",
    "    f1_score\n",
    ")\n",
    "from numpy.linalg import norm\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import sem\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d22612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Using device: cuda\n",
      "CUDA Available: True\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 60/320 [00:04<00:20, 12.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 343\u001b[0m\n\u001b[0;32m    339\u001b[0m     ce_loss, cov_penalty \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgnn\u001b[38;5;241m.\u001b[39m_debug_last_losses  \u001b[38;5;66;03m# store them in model during compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# print(f\"CE Loss: {ce_loss:.4f}, Cov Penalty: {cov_penalty:.4f}\") \u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    345\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_patient_graphs(EHR_binary, EHR_continuous, labels):\n",
    "    X_bin = EHR_binary.values\n",
    "    X_cont = EHR_continuous.values\n",
    "    num_patients, num_bin = X_bin.shape\n",
    "    _, num_cont = X_cont.shape\n",
    "    total_nodes = num_bin + num_cont\n",
    "\n",
    "    # --- Node features ---\n",
    "    # Binary: Bernoulli means\n",
    "    bin_event_probs = np.mean(X_bin, axis=0)\n",
    "    # Continuous: Z-score normalization\n",
    "    X_cont_z = zscore(X_cont, axis=0, ddof=0)  # shape: (num_patients, num_cont)\n",
    "\n",
    "    # --- Edge features (dependency/correlation matrix) ---\n",
    "    edge_weights = np.zeros((total_nodes, total_nodes))\n",
    "\n",
    "    # Combine all features column-wise\n",
    "    all_features = np.concatenate([X_bin, X_cont_z], axis=1)\n",
    "\n",
    "    edge_weights = np.zeros((total_nodes, total_nodes))\n",
    "    cosine_sims = []\n",
    "\n",
    "    # First pass: compute all edge weights\n",
    "    for i in range(total_nodes):\n",
    "        for j in range(total_nodes):\n",
    "            if i == j:\n",
    "                continue\n",
    "            xi = all_features[:, i]\n",
    "            xj = all_features[:, j]\n",
    "\n",
    "            if i < num_bin and j < num_bin:\n",
    "                # binary-binary: conditional P(xi=1 | xj=1)\n",
    "                numerator = np.sum((xi == 1) & (xj == 1))\n",
    "                denominator = np.sum(xj == 1)\n",
    "                edge_weights[i, j] = numerator / denominator if denominator != 0 else 0.0\n",
    "            else:\n",
    "                # Store raw cosine similarity to normalize later\n",
    "                cos_sim = np.dot(xi, xj) / (norm(xi) * norm(xj) + 1e-8)\n",
    "                cosine_sims.append((i, j, cos_sim))\n",
    "\n",
    "    # Normalize cosine similarities to [0, 1]\n",
    "    cos_vals = [val for _, _, val in cosine_sims]\n",
    "    cos_min = min(cos_vals)\n",
    "    cos_max = max(cos_vals)\n",
    "    for i, j, raw_cos in cosine_sims:\n",
    "        normalized_cos = (raw_cos - cos_min) / (cos_max - cos_min + 1e-8)\n",
    "        edge_weights[i, j] = normalized_cos\n",
    "\n",
    "\n",
    "\n",
    "    # --- Construct per-patient graphs ---\n",
    "    patient_graphs = []\n",
    "    for idx in range(num_patients):\n",
    "        bin_feats = np.where(X_bin[idx] == 1, bin_event_probs, 1 - bin_event_probs)\n",
    "        cont_feats = X_cont_z[idx]  # already normalized\n",
    "        node_features = np.concatenate([bin_feats, cont_feats])\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Edge index (fully connected, directed)\n",
    "        edge_index = torch.tensor(\n",
    "            [[i, j] for i in range(total_nodes) for j in range(total_nodes) if i != j],\n",
    "            dtype=torch.long\n",
    "        ).t().contiguous()\n",
    "\n",
    "        # Edge attributes\n",
    "        edge_attr = torch.tensor(\n",
    "            [edge_weights[i, j] for i in range(total_nodes) for j in range(total_nodes) if i != j],\n",
    "            dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        label_tensor = torch.tensor(labels[idx], dtype=torch.float32)\n",
    "\n",
    "        patient_graphs.append((node_features, edge_index, edge_attr, label_tensor))\n",
    "\n",
    "    return patient_graphs\n",
    "\n",
    "\n",
    "\n",
    "class EGraphSageLayer(nn.Module):\n",
    "    def __init__(self, in_dim_node, in_dim_edge, out_dim, num_negatives=5, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.message_linear = nn.Linear(in_dim_node + in_dim_edge, out_dim)\n",
    "        self.update_linear = nn.Linear(in_dim_node + out_dim, out_dim)\n",
    "\n",
    "        # Larger weight initialization for final layer\n",
    "        nn.init.xavier_uniform_(self.update_linear.weight, gain=2.0)\n",
    "        nn.init.constant_(self.update_linear.bias, 0.0)\n",
    "\n",
    "        self.num_negatives = num_negatives\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, node_feats, edge_index, edge_feats):\n",
    "        # node_feats.requires_grad_(True)\n",
    "        num_nodes = node_feats.size(0)\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "\n",
    "        dst_feats = node_feats[dst]\n",
    "        edge_feats_input = edge_feats[src]                    # (num_edges, node_dim)\n",
    "        \n",
    "        # edge_feats = edge_feats[src]  # Match source node count (num_edges)\n",
    "\n",
    "        edge_inputs = torch.cat([dst_feats, edge_feats_input], dim=1)\n",
    "        messages = F.leaky_relu(self.message_linear(edge_inputs), negative_slope=0.1) # (num_edges, 256)\n",
    "\n",
    "        agg = torch.zeros(num_nodes, messages.size(1), device=node_feats.device)\n",
    "        agg.index_add_(0, src, messages)\n",
    "\n",
    "        deg = torch.bincount(src, minlength=num_nodes).unsqueeze(1).clamp(min=1)\n",
    "        agg = agg / deg\n",
    "\n",
    "        node_inputs = torch.cat([node_feats, agg], dim=1)         # (num_nodes, node_dim + 256)\n",
    "        updated_nodes = F.leaky_relu(self.update_linear(node_inputs), negative_slope=0.1)         # (num_nodes, 256)\n",
    "\n",
    "        return updated_nodes\n",
    "\n",
    "\n",
    "class EGraphSageNet(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_dim=32, lr=1e-2, num_negatives=5):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(EGraphSageLayer(1, 1, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(EGraphSageLayer(hidden_dim, 1, hidden_dim))\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.num_negatives = num_negatives\n",
    "        self.classifier = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def compute_hybrid_loss(self, patient_embeddings, label_vec, lambda_cov=0.001):\n",
    "        \"\"\"\n",
    "        Combines supervised CrossEntropy loss with covariance penalty.\n",
    "        \"\"\"\n",
    "        device = patient_embeddings.device\n",
    "        graph_embedding = patient_embeddings.mean(dim=0)  # [hidden_dim]\n",
    "\n",
    "        # Step 2: Classify\n",
    "        logits = self.classifier(graph_embedding.unsqueeze(0))  # [1, 3]\n",
    "\n",
    "        # Step 3: Prepare target\n",
    "        label_tensor = torch.tensor([int(label_vec.item())], dtype=torch.long, device=logits.device)\n",
    "\n",
    "        # Step 4: Cross-Entropy Loss\n",
    "        ce_loss = F.cross_entropy(logits, label_tensor)\n",
    "\n",
    "\n",
    "        # Covariance Penalty\n",
    "        z_centered = patient_embeddings - patient_embeddings.mean(dim=0, keepdim=True)\n",
    "        cov = (z_centered.T @ z_centered) / (z_centered.size(0) - 1)\n",
    "        identity = torch.eye(cov.size(0)).to(device)\n",
    "        cov_penalty = torch.norm(cov - identity, p='fro')\n",
    "\n",
    "        total_loss = ce_loss + lambda_cov * cov_penalty\n",
    "\n",
    "        self._debug_last_losses = (ce_loss.item(), cov_penalty.item())\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def forward(self, node_feats, edge_index, edge_feats):\n",
    "        for layer in self.layers:\n",
    "            node_feats = layer(node_feats, edge_index, edge_feats)\n",
    "        return node_feats\n",
    "    \n",
    "    def compute_unsupervised_loss(self, node_embeddings, edge_index):\n",
    "        pos_loss = 0.0\n",
    "        neg_loss = 0.0\n",
    "        num_edges = edge_index.size(1)\n",
    "        device = node_embeddings.device\n",
    "        eps = 1e-8\n",
    "\n",
    "        norms = torch.norm(node_embeddings, dim=1, keepdim=True) + eps\n",
    "        normed_embeddings = node_embeddings / norms\n",
    "\n",
    "        for idx in range(num_edges):\n",
    "            src = edge_index[0, idx]\n",
    "            dst = edge_index[1, idx]\n",
    "            z_i = normed_embeddings[src]\n",
    "            z_j = normed_embeddings[dst]\n",
    "\n",
    "            dot_pos = torch.dot(z_i, z_j)\n",
    "            pos_loss += -torch.log(torch.sigmoid(dot_pos) + eps)\n",
    "\n",
    "            all_sims = torch.matmul(normed_embeddings, z_i)\n",
    "            all_sims[src] = 1.0\n",
    "            all_sims[dst] = 1.0\n",
    "            _, hard_neg_indices = torch.topk(-all_sims, self.num_negatives)\n",
    "\n",
    "            for neg_idx in hard_neg_indices:\n",
    "                z_k = normed_embeddings[neg_idx]\n",
    "                dot_neg = torch.dot(z_i, z_k)\n",
    "                neg_loss += -torch.log(torch.sigmoid(-dot_neg) + eps)\n",
    "\n",
    "        total_loss = (pos_loss + neg_loss) / (num_edges + eps)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class EGraphSageClassifier(nn.Module):\n",
    "    def __init__(self, num_layers=3, hidden_dim=32, num_outputs=3, num_events=None):\n",
    "        super().__init__()\n",
    "        self.gnn = EGraphSageNet(num_layers=num_layers, hidden_dim=hidden_dim)\n",
    "        # self.output_layer = nn.Linear(hidden_dim * num_events, num_outputs)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, node_feats, edge_index, edge_feats, return_embedding=False):\n",
    "        node_embeddings = self.gnn(node_feats, edge_index, edge_feats)  # shape: (M, hidden_dim)\n",
    "        # z_Gi = node_embeddings.flatten()  # shape: (M * hidden_dim,)\n",
    "        z_Gi = node_embeddings.mean(dim=0)\n",
    "\n",
    "        logits = self.gnn.classifier(z_Gi.unsqueeze(0)).squeeze(0)  # use trained classifier\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "\n",
    "        if return_embedding:\n",
    "            return probs, z_Gi  # Return both\n",
    "        else:\n",
    "            return probs  # Standard use\n",
    "\n",
    "# Convert the list of graph data to a Dataset-compatible structure\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graph_list):\n",
    "        self.graph_list = graph_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph_list[idx]\n",
    "\n",
    "# Collate function to batch graphs (assumes one graph per sample)\n",
    "def collate_fn(batch):\n",
    "    return batch  # keep it simple for now since batching full graphs may not be supported\n",
    "\n",
    "# === Load Data ===\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_pickle(\"df_encoded_files/df_encoded_final_with_outcome.pkl\")\n",
    "df = df.drop(columns=[\"ID\"])\n",
    "X = df.drop(columns=[\"Dropout Status\"])\n",
    "y = df[\"Dropout Status\"]\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "binary_cols = [col for col in X.columns if set(X[col].dropna().unique()).issubset({0, 1})]\n",
    "continuous_cols = [col for col in X.columns if col not in binary_cols]\n",
    "\n",
    "# X_binary = X[binary_cols]\n",
    "# X_continuous = X[continuous_cols]\n",
    "\n",
    "X_train_binary = X_train[binary_cols]\n",
    "X_train_cont = X_train[continuous_cols]\n",
    "X_test_binary = X_test[binary_cols]\n",
    "X_test_cont = X_test[continuous_cols]\n",
    "\n",
    "# === Apply SMOTE to training data ===\n",
    "X_train_combined = pd.concat([X_train_binary, X_train_cont], axis=1)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_resampled, y_train_resampled, test_size=0.2, stratify=y_train_resampled, random_state=42\n",
    ")\n",
    "\n",
    "X_train_binary_resampled = X_train_split[binary_cols]\n",
    "X_train_cont_resampled = X_train_split[continuous_cols]\n",
    "X_val_binary = X_val_split[binary_cols]\n",
    "X_val_cont = X_val_split[continuous_cols]\n",
    "\n",
    "\n",
    "train_graphs_with_labels = generate_patient_graphs(\n",
    "    X_train_binary_resampled, X_train_cont_resampled, y_train_split\n",
    ")\n",
    "val_graphs_with_labels = generate_patient_graphs(\n",
    "    X_val_binary, X_val_cont, y_val_split\n",
    ")\n",
    "test_graphs_with_labels = generate_patient_graphs(\n",
    "    X_test_binary, X_test_cont, y_test\n",
    ")\n",
    "\n",
    "\n",
    "all_classes = np.array([0, 1, 2])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Repeated 5-Fold CV ===\n",
    "gnn_scores_all = []\n",
    "\n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "num_events = X.shape[1]\n",
    "# Generate patient graphs\n",
    "# train_graphs_with_labels = generate_patient_graphs(X_binary, X_continuous, y_encoded)\n",
    "\n",
    "# dataset = GraphDataset(train_graphs_with_labels)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "train_dataset = GraphDataset(train_graphs_with_labels)\n",
    "val_dataset = GraphDataset(val_graphs_with_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model = EGraphSageClassifier(num_layers=3, hidden_dim=32, num_outputs=3, num_events=num_events).to(device)\n",
    "model.train()\n",
    "optimizer = model.gnn.optimizer\n",
    "num_epochs = 500\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0\n",
    "        for node_feats, edge_index, edge_feats, label_vec in batch:\n",
    "            node_feats = node_feats.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_feats = edge_feats.to(device)\n",
    "            node_embeddings = model.gnn(node_feats, edge_index, edge_feats)\n",
    "            loss = model.gnn.compute_hybrid_loss(node_embeddings, label_vec)\n",
    "            batch_loss += loss\n",
    "            \n",
    "            ce_loss, cov_penalty = model.gnn._debug_last_losses  # store them in model during compute\n",
    "\n",
    "        # print(f\"CE Loss: {ce_loss:.4f}, Cov Penalty: {cov_penalty:.4f}\") \n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    # === Compute validation loss ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            for node_feats, edge_index, edge_feats, label_vec in batch:\n",
    "                node_feats = node_feats.to(device)\n",
    "                edge_index = edge_index.to(device)\n",
    "                edge_feats = edge_feats.to(device)\n",
    "                node_embeddings = model.gnn(node_feats, edge_index, edge_feats)\n",
    "                loss = model.gnn.compute_hybrid_loss(node_embeddings, label_vec)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # === Check for improvement ===\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")  # Save best model\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # === Step the LR scheduler ===\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # === Early stopping condition ===\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    # for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "    #     optimizer.zero_grad()  # ✅ Clear previous gradients before new step\n",
    "\n",
    "    #     batch_loss = 0.0\n",
    "    #     for node_feats, edge_index, edge_feats, label_vec in batch:\n",
    "    #         node_feats = node_feats.to(device)\n",
    "    #         edge_index = edge_index.to(device)\n",
    "    #         edge_feats = edge_feats.to(device)\n",
    "\n",
    "    #         node_embeddings = model.gnn(node_feats, edge_index, edge_feats)\n",
    "    #         loss = model.gnn.compute_unsupervised_loss(node_embeddings, edge_index)\n",
    "    #         batch_loss += loss\n",
    "\n",
    "    #     batch_loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     total_loss += batch_loss.item()\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1} Training Loss: {total_loss:.4f}\")\n",
    "\n",
    "# === Embedding Extraction ===\n",
    "def extract_embeddings_and_predictions(model, graph_list):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    probs_all = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for node_feats, edge_index, edge_feats, label_vec in graph_list:\n",
    "            node_feats = node_feats.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_feats = edge_feats.to(device)\n",
    "            label_vec = label_vec.to(device)\n",
    "\n",
    "            probs, embedding = model(node_feats, edge_index, edge_feats, return_embedding=True)\n",
    "            pred_class = torch.argmax(probs).item()\n",
    "\n",
    "            embeddings.append(embedding.cpu().numpy())\n",
    "            probs_all.append(probs.cpu().numpy())\n",
    "            preds.append(pred_class)\n",
    "            labels.append(label_vec.cpu().item())\n",
    "\n",
    "    X_embeds = np.vstack(embeddings)\n",
    "    y_labels = np.array(labels)\n",
    "    y_preds = np.array(preds)\n",
    "    y_probs = np.vstack(probs_all)\n",
    "\n",
    "    return X_embeds, y_labels, y_preds, y_probs\n",
    "\n",
    "# === Train Set ===\n",
    "X_train_embeds, y_train_labels, y_train_preds, y_train_probs = extract_embeddings_and_predictions(model, train_graphs_with_labels)\n",
    "np.save(\"saved_embeddings/X_train_embeddings.npy\", X_train_embeds)\n",
    "np.save(\"saved_embeddings/y_train_labels.npy\", y_train_labels)\n",
    "\n",
    "# === Validation Set ===\n",
    "X_val_embeds, y_val_labels, y_val_preds, y_val_probs = extract_embeddings_and_predictions(model, val_graphs_with_labels)\n",
    "np.save(\"saved_embeddings/X_val_embeddings.npy\", X_val_embeds)\n",
    "np.save(\"saved_embeddings/y_val_labels.npy\", y_val_labels)\n",
    "\n",
    "\n",
    "# === Test Set ===\n",
    "X_test_embeds, y_test_labels, y_test_preds, y_test_probs = extract_embeddings_and_predictions(model, test_graphs_with_labels)\n",
    "np.save(\"saved_embeddings/X_test_embeddings.npy\", X_test_embeds)\n",
    "np.save(\"saved_embeddings/y_test_labels.npy\", y_test_labels)\n",
    "\n",
    "# === Evaluation on Test Set ===\n",
    "print(\"\\nEvaluation on Test Set:\")\n",
    "acc = accuracy_score(y_test_labels, y_test_preds)\n",
    "f1 = f1_score(y_test_labels, y_test_preds, average='macro')\n",
    "roc_auc = roc_auc_score(y_test_labels, y_test_probs, multi_class='ovr')\n",
    "pr_auc = average_precision_score(y_test_labels, y_test_probs, average='macro')\n",
    "cm = confusion_matrix(y_test_labels, y_test_preds)\n",
    "\n",
    "print(f\"Accuracy       : {acc:.4f}\")\n",
    "print(f\"F1 Score       : {f1:.4f}\")\n",
    "print(f\"ROC AUC        : {roc_auc:.4f}\")\n",
    "print(f\"AUC-PR         : {pr_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0598b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum variance: 0.0012309300946071744\n",
      "Maximum variance: 6.2326250076293945\n",
      "Number of features with variance < 0.01: 15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "X_train = np.load(\"saved_embeddings/X_train_embeddings.npy\")\n",
    "\n",
    "# Compute variance for each feature (column)\n",
    "variances = np.var(X_train, axis=0)\n",
    "\n",
    "# Calculate min, max, and count of low-variance features\n",
    "min_var = np.min(variances)\n",
    "max_var = np.max(variances)\n",
    "num_low_var = np.sum(variances < 0.01)\n",
    "\n",
    "# Print results\n",
    "print(f\"Minimum variance: {min_var}\")\n",
    "print(f\"Maximum variance: {max_var}\")\n",
    "print(f\"Number of features with variance < 0.01: {num_low_var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "474b45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "  Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "  Best Params: {'max_depth': 5, 'max_features': 0.8, 'n_estimators': 200}\n",
      "\n",
      "=== Two-Stage Classification: XGBoost ===\n",
      "--- Stage 1 Results (0 vs not-0) ---\n",
      "  Accuracy       : 0.5336\n",
      "  F1 Score       : 0.4096\n",
      "  ROC AUC        : 0.5322\n",
      "  AUC-PR         : 0.4514\n",
      "  Confusion Matrix:\n",
      "[[177  90]\n",
      " [132  77]]\n",
      "  Accuracy       : 0.4433\n",
      "  F1 Score       : 0.3057\n",
      "  ROC AUC        : 0.5048\n",
      "  AUC-PR         : 0.3314\n",
      "  Confusion Matrix:\n",
      "[[177  19  71]\n",
      " [ 54   4  28]\n",
      " [ 78  15  30]]\n",
      "\n",
      "=== Two-Stage Classification: Random Forest ===\n",
      "--- Stage 1 Results (0 vs not-0) ---\n",
      "  Accuracy       : 0.5567\n",
      "  F1 Score       : 0.1977\n",
      "  ROC AUC        : 0.5281\n",
      "  AUC-PR         : 0.4550\n",
      "  Confusion Matrix:\n",
      "[[239  28]\n",
      " [183  26]]\n",
      "  Accuracy       : 0.5231\n",
      "  F1 Score       : 0.2709\n",
      "  ROC AUC        : 0.5069\n",
      "  AUC-PR         : 0.3357\n",
      "  Confusion Matrix:\n",
      "[[239   5  23]\n",
      " [ 74   0  12]\n",
      " [109   4  10]]\n"
     ]
    }
   ],
   "source": [
    "# === Load saved data ===\n",
    "# X_train = np.load(\"saved_embeddings/X_train_embeddings.npy\")\n",
    "# y_train = np.load(\"saved_embeddings/y_train_labels.npy\")\n",
    "\n",
    "X_train = np.load(\"saved_embeddings/X_train_embeddings.npy\")\n",
    "y_train = np.load(\"saved_embeddings/y_train_labels.npy\")\n",
    "X_tune = np.load(\"saved_embeddings/X_val_embeddings.npy\")\n",
    "y_tune = np.load(\"saved_embeddings/y_val_labels.npy\")\n",
    "X_test = np.load(\"saved_embeddings/X_test_embeddings.npy\")\n",
    "y_test = np.load(\"saved_embeddings/y_test_labels.npy\")\n",
    "\n",
    "# === Split training into tuning + validation ===\n",
    "# X_tune, X_val, y_tune, y_val = train_test_split(\n",
    "#     X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    "# )\n",
    "\n",
    "# === Base Models ===\n",
    "base_models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "}\n",
    "\n",
    "# === Hyperparameter Grids ===\n",
    "param_grids = {\n",
    "    \"XGBoost\": {\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'max_depth': [5, 7],\n",
    "        'n_estimators': [100, 200],\n",
    "        'subsample': [0.6, 0.8]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [200, 300, 400],\n",
    "        'max_depth': [5, 7],\n",
    "        'max_features': [0.8, 0.95]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Grid Search ===\n",
    "best_params = {}\n",
    "for model_name, model in base_models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=3,\n",
    "        scoring='f1_macro',\n",
    "        verbose=1,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    grid.fit(X_tune, y_tune)\n",
    "    best_params[model_name] = grid.best_params_\n",
    "    print(f\"  Best Params: {grid.best_params_}\")\n",
    "\n",
    "# === Final Models ===\n",
    "final_models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        **best_params[\"XGBoost\"]\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        **best_params[\"Random Forest\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "for model_name, base_model in base_models.items():\n",
    "    print(f\"\\n=== Two-Stage Classification: {model_name} ===\")\n",
    "\n",
    "    # === Stage 1: Predict 0 vs not 0 ===\n",
    "    y_train_stage1 = np.where(y_train == 0, 0, 1)\n",
    "\n",
    "    model_stage1 = base_model.set_params(**best_params[model_name])\n",
    "    model_stage1.fit(X_train, y_train_stage1)\n",
    "    y_pred_stage1 = model_stage1.predict(X_test)\n",
    "    y_proba_stage1 = model_stage1.predict_proba(X_test)\n",
    "\n",
    "    # === Evaluate Stage 1: Binary classification (0 vs not-0) ===\n",
    "    y_test_stage1 = np.where(y_test == 0, 0, 1)\n",
    "\n",
    "    acc1 = accuracy_score(y_test_stage1, y_pred_stage1)\n",
    "    f1_1 = f1_score(y_test_stage1, y_pred_stage1, average='binary')\n",
    "    roc_auc1 = roc_auc_score(y_test_stage1, y_proba_stage1[:, 1])\n",
    "    pr_auc1 = average_precision_score(y_test_stage1, y_proba_stage1[:, 1])\n",
    "    cm1 = confusion_matrix(y_test_stage1, y_pred_stage1)\n",
    "\n",
    "    print(f\"--- Stage 1 Results (0 vs not-0) ---\")\n",
    "    print(f\"  Accuracy       : {acc1:.4f}\")\n",
    "    print(f\"  F1 Score       : {f1_1:.4f}\")\n",
    "    print(f\"  ROC AUC        : {roc_auc1:.4f}\")\n",
    "    print(f\"  AUC-PR         : {pr_auc1:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm1}\")\n",
    "\n",
    "\n",
    "    # === Stage 2: Predict 1 vs 2 for predicted non-0 ===\n",
    "    stage2_mask_train = y_train != 0\n",
    "    stage2_mask_test = y_pred_stage1 != 0\n",
    "\n",
    "    # Reinitialize model for Stage 2\n",
    "    if model_name == \"XGBoost\":\n",
    "        model_stage2 = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"mlogloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=42,\n",
    "            **best_params[\"XGBoost\"]\n",
    "        )\n",
    "    else:\n",
    "        model_stage2 = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            **best_params[\"Random Forest\"]\n",
    "        )\n",
    "\n",
    "    # Re-label: class 1 → 0, class 2 → 1 for binary classification\n",
    "    y_train_stage2 = np.where(y_train[stage2_mask_train] == 1, 0, 1)\n",
    "    model_stage2.fit(X_train[stage2_mask_train], y_train_stage2)\n",
    "\n",
    "    # Stage 2 predictions\n",
    "    y_pred_stage2_raw = model_stage2.predict(X_test[stage2_mask_test])\n",
    "    y_pred_stage2 = np.where(y_pred_stage2_raw == 0, 1, 2)\n",
    "    y_pred_final = np.copy(y_pred_stage1)\n",
    "    y_pred_final[stage2_mask_test] = y_pred_stage2\n",
    "\n",
    "    # === Construct 3-class probability matrix ===\n",
    "    y_proba_final = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    \n",
    "\n",
    "    # Fill stage 1 class 0 probabilities\n",
    "    y_proba_final[:, 0] = y_proba_stage1[:, 0]\n",
    "\n",
    "    # For samples predicted as not class 0:\n",
    "    # Assign class 1 and 2 probabilities from stage 2 model\n",
    "    y_proba_stage2 = model_stage2.predict_proba(X_test[stage2_mask_test])\n",
    "    # class 1 = stage2 class 0, class 2 = stage2 class 1\n",
    "    y_proba_final[stage2_mask_test, 1] = y_proba_stage1[stage2_mask_test, 1] * y_proba_stage2[:, 0]\n",
    "    y_proba_final[stage2_mask_test, 2] = y_proba_stage1[stage2_mask_test, 1] * y_proba_stage2[:, 1]\n",
    "\n",
    "    # Normalize rows to sum to 1\n",
    "    y_proba_final /= y_proba_final.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    # === Final 3-class metrics ===\n",
    "    acc = accuracy_score(y_test, y_pred_final)\n",
    "    f1 = f1_score(y_test, y_pred_final, average='macro')\n",
    "    roc_auc = roc_auc_score(y_test, y_proba_final, multi_class='ovr')\n",
    "    pr_auc = average_precision_score(y_test, y_proba_final, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred_final)\n",
    "\n",
    "    print(f\"  Accuracy       : {acc:.4f}\")\n",
    "    print(f\"  F1 Score       : {f1:.4f}\")\n",
    "    print(f\"  ROC AUC        : {roc_auc:.4f}\")\n",
    "    print(f\"  AUC-PR         : {pr_auc:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6e46dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of y_train_resampled (after SMOTE):\n",
      "Counter({np.float64(2.0): 1065, np.float64(0.0): 1065, np.float64(1.0): 1065})\n",
      "\n",
      "Distribution of y_test (original test set):\n",
      "Counter({np.float64(0.0): 267, np.float64(2.0): 123, np.float64(1.0): 86})\n",
      "\n",
      "y_train_resampled class percentages:\n",
      "Class 2: 1065 (33.33%)\n",
      "Class 0: 1065 (33.33%)\n",
      "Class 1: 1065 (33.33%)\n",
      "\n",
      "y_test class percentages:\n",
      "Class 0.0: 267 (56.09%)\n",
      "Class 2.0: 123 (25.84%)\n",
      "Class 1.0: 86 (18.07%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Print distribution using Counter\n",
    "print(\"Distribution of y_train_resampled (after SMOTE):\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\nDistribution of y_test (original test set):\")\n",
    "print(Counter(y_test))\n",
    "\n",
    "# Optionally, print percentages\n",
    "def print_percentages(y, name):\n",
    "    total = len(y)\n",
    "    counter = Counter(y)\n",
    "    print(f\"\\n{name} class percentages:\")\n",
    "    for cls, count in counter.items():\n",
    "        print(f\"Class {cls}: {count} ({count / total:.2%})\")\n",
    "\n",
    "print_percentages(y_train_resampled, \"y_train_resampled\")\n",
    "print_percentages(y_test, \"y_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd31862b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_embeddings/X_embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# === Load Embeddings and Labels ===\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_embeddings/X_embeddings.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_embeddings/y_labels.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert to proper format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\khang\\OneDrive\\Desktop\\SCHOOL\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_embeddings/X_embeddings.npy'"
     ]
    }
   ],
   "source": [
    "########Code that doesn't have RFE for Random Forest\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Load Embeddings and Labels ===\n",
    "X_embeddings = np.load(\"saved_embeddings/X_embeddings.npy\", allow_pickle=True)\n",
    "y_labels = np.load(\"saved_embeddings/y_labels.npy\", allow_pickle=True)\n",
    "\n",
    "# Convert to proper format\n",
    "X = np.vstack(X_embeddings).astype(np.float32)\n",
    "y = np.array(y_labels)\n",
    "\n",
    "# === Base Models for GridSearch ===\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# === Base Models for GridSearch ===\n",
    "base_models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced'   # ✅ Added here\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# === Train/Validation Split ===\n",
    "X_tune, X_holdout, y_tune, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Hyperparameter Grids ===\n",
    "param_grids = {\n",
    "    \"XGBoost\": {\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'max_depth': [5, 7],\n",
    "        'n_estimators': [100, 200],\n",
    "        'subsample': [0.6, 0.8]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [200, 300, 400],\n",
    "        'max_depth': [5, 7],\n",
    "        'max_features': [0.8, 0.95]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Grid Search for Best Parameters ===\n",
    "best_params = {}\n",
    "for model_name, model in base_models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=3,\n",
    "        scoring='f1_macro',\n",
    "        verbose=0,\n",
    "        n_jobs=4  \n",
    "    )\n",
    "    grid.fit(X_tune, y_tune)\n",
    "    best_params[model_name] = grid.best_params_\n",
    "    print(f\"  Best Params: {grid.best_params_}\")\n",
    "\n",
    "# === Final Models with Best Params ===\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        **best_params[\"XGBoost\"]\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        **best_params[\"Random Forest\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    try:\n",
    "        if y_prob.shape[1] == 2:\n",
    "            auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "            auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "        else:\n",
    "            auc_roc = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average='macro')\n",
    "            auc_pr = average_precision_score(y_true, y_prob, average='macro')\n",
    "    except:\n",
    "        auc_roc, auc_pr = np.nan, np.nan\n",
    "    return acc, f1, auc_roc, auc_pr\n",
    "\n",
    "# === Cross-Validation ===\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "results = {model_name: defaultdict(list) for model_name in models}\n",
    "\n",
    "all_y_true = {model_name: [] for model_name in models}\n",
    "all_y_pred = {model_name: [] for model_name in models}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    for train_idx, test_idx in rskf.split(X_holdout, y_holdout):\n",
    "        X_train, X_test = X_holdout[train_idx], X_holdout[test_idx]\n",
    "        y_train, y_test = y_holdout[train_idx], y_holdout[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        all_y_true[model_name].extend(y_test)\n",
    "        all_y_pred[model_name].extend(y_pred)\n",
    "\n",
    "\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        y_prob = (\n",
    "            model.predict_proba(X_test)\n",
    "            if hasattr(model, \"predict_proba\")\n",
    "            else np.eye(n_classes)[y_pred]\n",
    "        )\n",
    "\n",
    "        acc, f1, auc_roc, auc_pr = compute_metrics(y_test, y_prob, y_pred)\n",
    "        results[model_name]['Accuracy'].append(acc)\n",
    "        results[model_name]['F1 Score'].append(f1)\n",
    "        results[model_name]['AUC-ROC'].append(auc_roc)\n",
    "        results[model_name]['AUC-PR'].append(auc_pr)\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\nCross-Validated Performance (5x5-fold)\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric_name, scores in metrics.items():\n",
    "        mean_score = np.nanmean(scores)\n",
    "        stderr = sem(scores, nan_policy='omit')\n",
    "        print(f\"  {metric_name}: {mean_score:.4f} ± {stderr:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrices\")\n",
    "for model_name in models:\n",
    "    print(f\"\\n{model_name} Confusion Matrix:\")\n",
    "    cm = confusion_matrix(all_y_true[model_name], all_y_pred[model_name])\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c010087",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code that has RFE for Random Forest\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Load Embeddings and Labels ===\n",
    "X_embeddings = np.load(\"saved_embeddings/X_embeddings.npy\", allow_pickle=True)\n",
    "y_labels = np.load(\"saved_embeddings/y_labels.npy\", allow_pickle=True)\n",
    "\n",
    "# Convert to proper format\n",
    "X = np.vstack(X_embeddings).astype(np.float32)\n",
    "y = np.array(y_labels)\n",
    "\n",
    "# === Base Models for GridSearch ===\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# === Base Models for GridSearch ===\n",
    "base_models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced'   # ✅ Added here\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# === Train/Validation Split ===\n",
    "X_tune, X_holdout, y_tune, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Hyperparameter Grids ===\n",
    "param_grids = {\n",
    "    \"XGBoost\": {\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'max_depth': [5, 7],\n",
    "        'n_estimators': [100, 200],\n",
    "        'subsample': [0.6, 0.8]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [200, 300, 400],\n",
    "        'max_depth': [5, 7],\n",
    "        'max_features': [0.8, 0.95]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Grid Search for Best Parameters ===\n",
    "best_params = {}\n",
    "for model_name, model in base_models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=3,\n",
    "        scoring='f1_macro',\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_tune, y_tune)\n",
    "    best_params[model_name] = grid.best_params_\n",
    "    print(f\"  Best Params: {grid.best_params_}\")\n",
    "\n",
    "# === Final Models with Best Params ===\n",
    "# === Final Models with Best Params ===\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        **best_params[\"XGBoost\"]\n",
    "    ),\n",
    "    \"Random Forest\": RFE(  # ✅ Wrap RandomForest with RFE\n",
    "        estimator=RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            **best_params[\"Random Forest\"]\n",
    "        ),\n",
    "        n_features_to_select=40  # ✅ Select top 40 features (or change as needed)\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    try:\n",
    "        if y_prob.shape[1] == 2:\n",
    "            auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "            auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "        else:\n",
    "            auc_roc = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average='macro')\n",
    "            auc_pr = average_precision_score(y_true, y_prob, average='macro')\n",
    "    except:\n",
    "        auc_roc, auc_pr = np.nan, np.nan\n",
    "    return acc, f1, auc_roc, auc_pr\n",
    "\n",
    "# === Cross-Validation ===\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "results = {model_name: defaultdict(list) for model_name in models}\n",
    "\n",
    "all_y_true = {model_name: [] for model_name in models}\n",
    "all_y_pred = {model_name: [] for model_name in models}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    for train_idx, test_idx in rskf.split(X_holdout, y_holdout):\n",
    "        X_train, X_test = X_holdout[train_idx], X_holdout[test_idx]\n",
    "        y_train, y_test = y_holdout[train_idx], y_holdout[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        all_y_true[model_name].extend(y_test)\n",
    "        all_y_pred[model_name].extend(y_pred)\n",
    "\n",
    "\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        y_prob = (\n",
    "            model.predict_proba(X_test)\n",
    "            if hasattr(model, \"predict_proba\")\n",
    "            else np.eye(n_classes)[y_pred]\n",
    "        )\n",
    "\n",
    "        acc, f1, auc_roc, auc_pr = compute_metrics(y_test, y_prob, y_pred)\n",
    "        results[model_name]['Accuracy'].append(acc)\n",
    "        results[model_name]['F1 Score'].append(f1)\n",
    "        results[model_name]['AUC-ROC'].append(auc_roc)\n",
    "        results[model_name]['AUC-PR'].append(auc_pr)\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\nCross-Validated Performance (5x5-fold)\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric_name, scores in metrics.items():\n",
    "        mean_score = np.nanmean(scores)\n",
    "        stderr = sem(scores, nan_policy='omit')\n",
    "        print(f\"  {metric_name}: {mean_score:.4f} ± {stderr:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrices\")\n",
    "for model_name in models:\n",
    "    print(f\"\\n{model_name} Confusion Matrix:\")\n",
    "    cm = confusion_matrix(all_y_true[model_name], all_y_pred[model_name])\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6034482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min of X_embeddings: -0.2504233717918396\n",
      "Max of X_embeddings: 3.1098434925079346\n",
      "Number of features with variance below 0.01: 23\n",
      "Number of features: 32\n",
      "Number of zeros: 0\n",
      "Total elements: 76160\n",
      "Percentage of zeros: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load embeddings and labels\n",
    "X_embeddings = np.load(\"saved_embeddings/X_embeddings.npy\")\n",
    "y_labels = np.load(\"saved_embeddings/y_labels.npy\")\n",
    "\n",
    "# Calculate min and max of embeddings\n",
    "X_min = X_embeddings.min()\n",
    "X_max = X_embeddings.max()\n",
    "print(f\"Min of X_embeddings: {X_min}\")\n",
    "print(f\"Max of X_embeddings: {X_max}\")\n",
    "\n",
    "# Calculate variance of each feature\n",
    "variances = np.var(X_embeddings, axis=0)\n",
    "\n",
    "# Count number of features with variance below 0.01\n",
    "low_variance_count = np.sum(variances < 0.01)\n",
    "print(f\"Number of features with variance below 0.01: {low_variance_count}\")\n",
    "\n",
    "num_features = X_embeddings.shape[1]\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_zeros = np.sum(X_embeddings == 0)\n",
    "total_elements = X_embeddings.size\n",
    "percent_zeros = (num_zeros / total_elements) * 100\n",
    "\n",
    "print(f\"Number of zeros: {num_zeros}\")\n",
    "print(f\"Total elements: {total_elements}\")\n",
    "print(f\"Percentage of zeros: {percent_zeros:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5f6ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load labels\n",
    "y_labels = np.load(\"saved_embeddings/y_labels.npy\")\n",
    "\n",
    "# Count class frequencies\n",
    "class_counts = Counter(y_labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution in y_labels\")\n",
    "plt.xticks(sorted(class_counts.keys()))\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
