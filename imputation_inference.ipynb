{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing percentages (based on 0 values) for each column:\n",
      "SYSTOLIC_BP_min      3.684608\n",
      "SYSTOLIC_BP_max      3.684608\n",
      "SYSTOLIC_BP_mean     3.684608\n",
      "DIASTOLIC_BP_min     3.684608\n",
      "DIASTOLIC_BP_max     3.684608\n",
      "DIASTOLIC_BP_mean    3.684608\n",
      "dtype: float64\n",
      "                    Variable  Missing_Percentage\n",
      "0       PRIMARY_RACE_Unknown            7.230885\n",
      "1           LANGUAGE_Unknown            0.213783\n",
      "2  PRIMARY_ETHNICITY_Unknown           26.810865\n",
      "3    D_Insur_at_pull_Unknown            0.968310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nVariable  Missing_Percentage\\n0       PRIMARY_RACE_Unknown            5.985169\\n1           LANGUAGE_Unknown            0.123588\\n2  PRIMARY_ETHNICITY_Unknown           20.533192\\n3    D_Insur_at_pull_Unknown            0.052966\\n\\n0             min_BMI            1.906780\\n1          min_HEIGHT            1.553672\\n2           min_PULSE            2.454096\\n3          min_WEIGHT            0.141243\\n4             max_BMI            1.889124\\n5          max_HEIGHT            1.553672\\n6           max_PULSE            0.017655\\n7          max_WEIGHT            0.141243\\n8            mean_BMI            1.889124\\n9         mean_HEIGHT            1.553672\\n10         mean_PULSE            0.017655\\n11        mean_WEIGHT            0.141243\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the missing percentages of all missing variables in data\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"Temp!!!\"\"\"\n",
    "df_encoded = pd.read_pickle(\"df_encoded_files/df_encoded.pkl\")\n",
    "\n",
    "\n",
    "# Define the columns to replace NaN values with 0\n",
    "columns_to_replace = [\n",
    "    \"SYSTOLIC_BP_min\", \"SYSTOLIC_BP_max\", \"SYSTOLIC_BP_mean\",\n",
    "    \"DIASTOLIC_BP_min\", \"DIASTOLIC_BP_max\", \"DIASTOLIC_BP_mean\"\n",
    "]\n",
    "\n",
    "# Replace NaN values with 0 in the specified columns\n",
    "df_encoded[columns_to_replace] = df_encoded[columns_to_replace].fillna(0)\n",
    "\n",
    "# Compute the percentage of missing values (i.e., values that are 0) for the specified columns\n",
    "missing_percentages = (df_encoded[columns_to_replace] == 0).mean() * 100\n",
    "\n",
    "# Display the missing percentages\n",
    "print(\"Missing percentages (based on 0 values) for each column:\")\n",
    "print(missing_percentages)\n",
    "\n",
    "# List of variables to check\n",
    "variables_to_check_0 = [\n",
    "    'min_BMI', 'min_HEIGHT', 'min_PULSE', 'min_WEIGHT', \n",
    "    'max_BMI', 'max_HEIGHT', 'max_PULSE', 'max_WEIGHT', \n",
    "    'mean_BMI', 'mean_HEIGHT', 'mean_PULSE', 'mean_WEIGHT',\n",
    "    'SYSTOLIC_BP_min', 'SYSTOLIC_BP_max', 'SYSTOLIC_BP_mean',\n",
    "    'DIASTOLIC_BP_min', 'DIASTOLIC_BP_max', 'DIASTOLIC_BP_mean',\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "variables_to_check_1 = [\n",
    "    'PRIMARY_RACE_Unknown', \n",
    "    'LANGUAGE_Unknown',\n",
    "    'PRIMARY_ETHNICITY_Unknown',\n",
    "    'D_Insur_at_pull_Unknown',\n",
    "]\n",
    "\n",
    "# Calculate missing percentages (where value == 0)\n",
    "missing_percentages = (df_encoded[variables_to_check_0] == 0).mean() * 100\n",
    "\n",
    "# Calculate missing percentages (where value == 1)\n",
    "missing_percentages = (df_encoded[variables_to_check_1] == 1).mean() * 100\n",
    "\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "missing_df = missing_percentages.reset_index()\n",
    "missing_df.columns = ['Variable', 'Missing_Percentage']\n",
    "\n",
    "# Print the missing percentages\n",
    "print(missing_df)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Variable  Missing_Percentage\n",
    "0       PRIMARY_RACE_Unknown            5.985169\n",
    "1           LANGUAGE_Unknown            0.123588\n",
    "2  PRIMARY_ETHNICITY_Unknown           20.533192\n",
    "3    D_Insur_at_pull_Unknown            0.052966\n",
    "\n",
    "0             min_BMI            1.906780\n",
    "1          min_HEIGHT            1.553672\n",
    "2           min_PULSE            2.454096\n",
    "3          min_WEIGHT            0.141243\n",
    "4             max_BMI            1.889124\n",
    "5          max_HEIGHT            1.553672\n",
    "6           max_PULSE            0.017655\n",
    "7          max_WEIGHT            0.141243\n",
    "8            mean_BMI            1.889124\n",
    "9         mean_HEIGHT            1.553672\n",
    "10         mean_PULSE            0.017655\n",
    "11        mean_WEIGHT            0.141243\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Specified object-type columns successfully converted to 0 and 1.\n",
      "PRIMARY_RACE_American Indian / Native American     int64\n",
      "PRIMARY_RACE_Asian                                 int64\n",
      "PRIMARY_RACE_Asian Indian                          int64\n",
      "PRIMARY_RACE_Black / African American              int64\n",
      "PRIMARY_RACE_Middle Eastern                        int64\n",
      "PRIMARY_RACE_Native Hawaiian / Pacific Islander    int64\n",
      "PRIMARY_RACE_White                                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_encoded = pd.read_pickle(\"df_encoded_files/df_encoded.pkl\")\n",
    "\n",
    "# Convert boolean data types to integers\n",
    "df_encoded = df_encoded.astype({col: int for col in df_encoded.select_dtypes(include=['bool']).columns})\n",
    "\n",
    "# Identify columns that start with 'PRIMARY_RACE' but exclude 'PRIMARY_RACE_Unknown'\n",
    "race_columns = [col for col in df_encoded.columns if col.startswith(\"PRIMARY_RACE\") and col != \"PRIMARY_RACE_Unknown\"]\n",
    "\n",
    "# Set 'PRIMARY_RACE_Unknown' to 1 if all other 'PRIMARY_RACE' columns are 0\n",
    "df_encoded.loc[df_encoded[race_columns].sum(axis=1) == 0, \"PRIMARY_RACE_Unknown\"] = 1\n",
    "\n",
    "unknown_ethnicity_list = []\n",
    "\n",
    "# Filter the dataframe to get rows where PRIMARY_ETHNICITY_Unknown == 1\n",
    "filtered_rows = df_encoded[df_encoded['PRIMARY_ETHNICITY_Unknown'] == 1]\n",
    "\n",
    "i = 0\n",
    "# Print the row index (from df) along with the ID column\n",
    "for index, row in filtered_rows.iterrows():\n",
    "    # if i <= 25:\n",
    "    #     print(f\"Row Index: {index}, ID: {row['ID']}\")\n",
    "    #     i += 1\n",
    "    unknown_ethnicity_list.append(index)\n",
    "\n",
    "\n",
    "unknown_race_list = []\n",
    "\n",
    "# Filter the dataframe to get rows where PRIMARY_RACE_Unknown == 1\n",
    "filtered_rows = df_encoded[df_encoded['PRIMARY_RACE_Unknown'] == 1]\n",
    "\n",
    "# print('='*50)\n",
    "i = 0\n",
    "# Print the row index (from df) along with the ID column\n",
    "for index, row in filtered_rows.iterrows():\n",
    "    # if i <= 25:\n",
    "    #     print(f\"Row Index: {index}, ID: {row['ID']}\")\n",
    "    #     i += 1\n",
    "    unknown_race_list.append(index)\n",
    "\n",
    "# List of columns that need to be converted\n",
    "columns_to_convert = [\n",
    "    'PRIMARY_RACE_American Indian / Native American',\n",
    "    'PRIMARY_RACE_Asian',\n",
    "    'PRIMARY_RACE_Asian Indian',\n",
    "    'PRIMARY_RACE_Black / African American',\n",
    "    'PRIMARY_RACE_Middle Eastern',\n",
    "    'PRIMARY_RACE_Native Hawaiian / Pacific Islander',\n",
    "    'PRIMARY_RACE_White'\n",
    "]\n",
    "\n",
    "# Ensure only existing columns are processed\n",
    "existing_columns = [col for col in columns_to_convert if col in df_encoded.columns]\n",
    "\n",
    "# Convert string-based boolean values to actual integers (0 and 1), handling NaNs\n",
    "df_encoded[existing_columns] = df_encoded[existing_columns].apply(\n",
    "    lambda col: col.astype(str).str.lower().map({'false': 0, 'true': 1}).fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# Verify that conversion was successful\n",
    "print(\"✅ Specified object-type columns successfully converted to 0 and 1.\")\n",
    "print(df_encoded[existing_columns].dtypes)  # Should all be int64 or int32\n",
    "\n",
    "df_encoded[\"RPL_THEME1\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Impute datas are are barely missing\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Define the columns to impute\n",
    "columns_to_impute = [\n",
    "    \"min_BMI\", \"min_HEIGHT\", \"min_PULSE\", \"min_WEIGHT\",\n",
    "    \"max_BMI\", \"max_HEIGHT\", \"max_PULSE\", \"max_WEIGHT\",\n",
    "    \"mean_BMI\", \"mean_HEIGHT\", \"mean_PULSE\", \"mean_WEIGHT\",\n",
    "    \"SYSTOLIC_BP_min\", \"SYSTOLIC_BP_max\", \"SYSTOLIC_BP_mean\",\n",
    "    \"DIASTOLIC_BP_min\", \"DIASTOLIC_BP_max\", \"DIASTOLIC_BP_mean\"\n",
    "]\n",
    "\n",
    "# Create a copy of the dataset to avoid modifying the original\n",
    "df_knn_impute = df_encoded.copy()\n",
    "\n",
    "# Initialize KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply KNN Imputation only to selected columns\n",
    "df_knn_impute[columns_to_impute] = knn_imputer.fit_transform(df_knn_impute[columns_to_impute])\n",
    "\n",
    "# Replace only the NaN values in df_encoded with the imputed values\n",
    "df_encoded[columns_to_impute] = df_encoded[columns_to_impute].where(df_encoded.notna(), df_knn_impute[columns_to_impute])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s in RPL_THEME1: 1152\n",
      "Number of blank (NaN) values in RPL_THEME1: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for 0s\n",
    "zero_count = (df_encoded[\"RPL_THEME1\"] == 0).sum()\n",
    "\n",
    "# Check for blank (NaN) values\n",
    "nan_count = df_encoded[\"RPL_THEME1\"].isna().sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of 0s in RPL_THEME1: {zero_count}\")\n",
    "print(f\"Number of blank (NaN) values in RPL_THEME1: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imputation complete! _Unknown columns are preserved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "# Suppress FutureWarnings globally\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# 🔹 I-NAAutoencoder Model with Undercomplete Representation and Dropout\n",
    "class INA_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(INA_Autoencoder, self).__init__()\n",
    "\n",
    "        # Undercomplete Representation: Smaller hidden layers than input\n",
    "        hidden_dim1 = int(input_dim * 0.75)  # Compress to 75% of input size\n",
    "        hidden_dim2 = int(input_dim * 0.60)  # Compress to 60% of input size\n",
    "        hidden_dim3 = int(input_dim * 0.5)   # Compress to 50% of input size\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim1, input_dim),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for both continuous and binary features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in input\")\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        if torch.isnan(encoded).any():\n",
    "            print(\"NaN detected in encoding\")\n",
    "\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        if torch.isnan(decoded).any():\n",
    "            print(\"NaN detected in output\")\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Define the columns to remove before imputation\n",
    "unknown_columns = [\n",
    "    'PRIMARY_ETHNICITY_Unknown',\n",
    "    'PRIMARY_RACE_Unknown',\n",
    "    'LANGUAGE_Unknown',\n",
    "    'D_Insur_at_pull_Unknown',\n",
    "]\n",
    "\n",
    "# 🔹 Create a copy of df_encoded to avoid modifying original data\n",
    "df_impute = df_encoded.drop(columns=unknown_columns, errors='ignore').copy()\n",
    "df_impute = df_impute.drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "\n",
    "# 🔹 Load the trained I-NAA model\n",
    "model_path = \"best_inaa_models/best_INAA_MAR_30.pth\"\n",
    "\n",
    "\n",
    "# Initialize the model with the correct input size (based on df_impute)\n",
    "input_dim = df_impute.shape[1]\n",
    "model = INA_Autoencoder(input_dim)  # Ensure the model matches feature count\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 🔹 Identify rows where any of the _Unknown columns are 1 (indicating missing categorical values)\n",
    "missing_rows = (df_encoded['RPL_THEME1'] == 0) | df_encoded['RPL_THEME1'].isna()\n",
    "\n",
    "# 🔹 Prepare data for imputation\n",
    "input_data = torch.tensor(df_impute.loc[missing_rows].values, dtype=torch.float32).to(device)\n",
    "\n",
    "# 🔹 Perform imputation with the trained model\n",
    "with torch.no_grad():\n",
    "    imputed_data = model(input_data).cpu().numpy()  # Move back to CPU\n",
    "\n",
    "# 🔹 Replace only the missing rows in the original dataset with imputed values\n",
    "df_impute.loc[missing_rows, df_impute.columns] = imputed_data\n",
    "\n",
    "# ✅ Successfully imputed missing categorical values while keeping _Unknown columns intact!\n",
    "print(\"✅ Imputation complete! _Unknown columns are preserved.\")\n",
    "\n",
    "df_encoded.loc[missing_rows, \"RPL_THEME1\"] = df_impute.loc[missing_rows, \"RPL_THEME1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_pickle(\"df_encoded_imputed_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Once imputation training is done, we first run all the cells like usual and run the imputation model that is 30% missing on MAR \n",
    "use that to impute our ethnicities. after that just rerun the training code cell but change the model to use the one that is 10% \n",
    "missing on MAR. do the same thing but this time we change the values for race. after we impute all our values into df_encoded we \n",
    "save it as df_encoded_imputed.pkl\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
