{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed: ['min_BMC_ALT(SGPT)', 'min_BMC_AST(SGOT)', 'min_BMC_GLUCOSE', 'min_CREATININE', 'max_BMC_ALT(SGPT)', 'max_BMC_AST(SGOT)', 'max_BMC_GLUCOSE', 'max_CREATININE', 'mean_BMC_ALT(SGPT)', 'mean_BMC_AST(SGOT)', 'mean_BMC_GLUCOSE', 'mean_CREATININE']\n",
      "demo_age        int64\n",
      "RPL_THEME1    float64\n",
      "GENDER_F        int64\n",
      "GENDER_M        int64\n",
      "GENDER_U        int64\n",
      "               ...   \n",
      "F41.8           int64\n",
      "F41.0           int64\n",
      "F33.2           int64\n",
      "F20.89          int64\n",
      "F34.1           int64\n",
      "Length: 163, dtype: object\n",
      "Number of rows in df_encoded: 5664\n",
      "Number of rows in df_complete: 4205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_encoded = pd.read_pickle(\"df_encoded.pkl\")\n",
    "\n",
    "# Drop 'ID' column if it exists\n",
    "id_column = \"ID\" if \"ID\" in df_encoded.columns else None\n",
    "if id_column:\n",
    "    df_encoded = df_encoded.drop(columns=[id_column])\n",
    "    \n",
    "\"\"\"Remove these columns:\n",
    "CREATININE \n",
    "BMC_GLUCOSE \n",
    "BMC_ALT(SGPT) \n",
    "BMC_AST(SGOT)\n",
    "\"\"\"\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_remove = ['min_BMC_ALT(SGPT)', 'min_BMC_AST(SGOT)', 'min_BMC_GLUCOSE', \n",
    "    'min_CREATININE', 'max_BMC_ALT(SGPT)', 'max_BMC_AST(SGOT)', \n",
    "    'max_BMC_GLUCOSE', 'max_CREATININE', 'mean_BMC_ALT(SGPT)', \n",
    "    'mean_BMC_AST(SGOT)', 'mean_BMC_GLUCOSE', 'mean_CREATININE']\n",
    "\n",
    "# Drop the columns from df_encoded (ignore errors if a column doesn't exist)\n",
    "df_encoded = df_encoded.drop(columns=columns_to_remove, errors=\"ignore\")\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Columns removed:\", columns_to_remove)\n",
    "\n",
    "\n",
    "# Make a copy of df_encoded\n",
    "df_complete = df_encoded.copy()\n",
    "\n",
    "# Step 1: Remove rows where any of the specified variables have a 1\n",
    "variables_to_check_1 = [\n",
    "    'PRIMARY_RACE_Unknown', \n",
    "    'LANGUAGE_Unknown',\n",
    "    'PRIMARY_ETHNICITY_Unknown',\n",
    "    'D_Insur_at_pull_Unknown'\n",
    "]\n",
    "\n",
    "df_complete = df_complete[~df_complete[variables_to_check_1].eq(1).any(axis=1)]\n",
    "\n",
    "# Step 2: Remove rows where any of the specified variables have a 0\n",
    "# variables_to_check_0 = [\n",
    "    \n",
    "#     'min_BMI', 'min_HEIGHT', 'min_PULSE', 'min_WEIGHT', \n",
    "#     'max_BMI', 'max_HEIGHT', 'max_PULSE', 'max_WEIGHT', \n",
    "#     'mean_BMI', 'mean_HEIGHT', 'mean_PULSE', 'mean_WEIGHT', \n",
    "#     'min_BMC_ALT(SGPT)', 'min_BMC_AST(SGOT)', 'min_BMC_GLUCOSE', \n",
    "#     'min_CREATININE', 'max_BMC_ALT(SGPT)', 'max_BMC_AST(SGOT)', \n",
    "#     'max_BMC_GLUCOSE', 'max_CREATININE', 'mean_BMC_ALT(SGPT)', \n",
    "#     'mean_BMC_AST(SGOT)', 'mean_BMC_GLUCOSE', 'mean_CREATININE'\n",
    "# ]\n",
    "\n",
    "variables_to_check_0 = [\n",
    "    \n",
    "    'min_BMI', 'min_HEIGHT', 'min_PULSE', 'min_WEIGHT', \n",
    "    'max_BMI', 'max_HEIGHT', 'max_PULSE', 'max_WEIGHT', \n",
    "    'mean_BMI', 'mean_HEIGHT', 'mean_PULSE', 'mean_WEIGHT',\n",
    "    'SYSTOLIC_BP_min', 'SYSTOLIC_BP_max', 'SYSTOLIC_BP_mean',\n",
    "    'DIASTOLIC_BP_min', 'DIASTOLIC_BP_max', 'DIASTOLIC_BP_mean'\n",
    "]\n",
    "\n",
    "df_complete=  df_complete[~df_complete[variables_to_check_0].eq(0).any(axis=1)]\n",
    "\n",
    "# Step 3: Drop the four specified columns\n",
    "df_complete = df_complete.drop(columns=variables_to_check_1, errors='ignore')\n",
    "\n",
    "# Convert boolean data types to integers\n",
    "df_complete = df_complete.astype({col: int for col in df_complete.select_dtypes(include=['bool']).columns})\n",
    "\n",
    "# Confirm the change\n",
    "print(df_complete.dtypes)\n",
    "\n",
    "\n",
    "# Assuming df_complete is already defined\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize all features\n",
    "df_complete = pd.DataFrame(scaler.fit_transform(df_complete), columns=df_complete.columns)\n",
    "\n",
    "print(f\"Number of rows in df_encoded: {len(df_encoded)}\")\n",
    "print(f\"Number of rows in df_complete: {len(df_complete)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Columns: ['demo_age', 'RPL_THEME1', 'min_CLOZAPINE', 'min_OLANZAPINE', 'min_RISPERIDONE', 'max_CLOZAPINE', 'max_OLANZAPINE', 'max_RISPERIDONE', 'mean_CLOZAPINE', 'mean_OLANZAPINE', 'mean_RISPERIDONE', 'min_BMI', 'min_HEIGHT', 'min_PULSE', 'min_WEIGHT', 'max_BMI', 'max_HEIGHT', 'max_PULSE', 'max_WEIGHT', 'mean_BMI', 'mean_HEIGHT', 'mean_PULSE', 'mean_WEIGHT', 'SYSTOLIC_BP_min', 'SYSTOLIC_BP_max', 'SYSTOLIC_BP_mean', 'DIASTOLIC_BP_min', 'DIASTOLIC_BP_max', 'DIASTOLIC_BP_mean']\n",
      "Binary Columns: ['GENDER_F', 'GENDER_M', 'GENDER_U', 'PRIMARY_RACE_American Indian / Native American', 'PRIMARY_RACE_Asian', 'PRIMARY_RACE_Asian Indian', 'PRIMARY_RACE_Black / African American', 'PRIMARY_RACE_Hispanic or Latino', 'PRIMARY_RACE_Middle Eastern', 'PRIMARY_RACE_Native Hawaiian / Pacific Islander', 'PRIMARY_RACE_Other', 'PRIMARY_RACE_White', 'LANGUAGE_Afrikaans', 'LANGUAGE_Albanian', 'LANGUAGE_American Sign Language', 'LANGUAGE_American Sign language & Certified Deaf Interpreter', 'LANGUAGE_Amharic / Ethiopia', 'LANGUAGE_Arabic', 'LANGUAGE_Bassa / Liberia', 'LANGUAGE_Brazilian Portuguese', 'LANGUAGE_Cape Verdean / Port Creole', 'LANGUAGE_Chinese / Cantonese', 'LANGUAGE_Chinese / Mandarin', 'LANGUAGE_English', 'LANGUAGE_French', 'LANGUAGE_Fulani / Cameroon', 'LANGUAGE_Haitian Creole', 'LANGUAGE_Italian', 'LANGUAGE_Kirundi / Burundi', 'LANGUAGE_Luganda / Uganda', 'LANGUAGE_Nepali', 'LANGUAGE_Oromo / Ethiopia', 'LANGUAGE_Other', 'LANGUAGE_Polish', 'LANGUAGE_Portuguese', 'LANGUAGE_Romanian', 'LANGUAGE_Somali', 'LANGUAGE_Spanish', 'LANGUAGE_Tagalog', 'LANGUAGE_Tigrinya', 'LANGUAGE_Turkish', 'LANGUAGE_Ukrainian', 'LANGUAGE_Vietnamese', 'PRIMARY_ETHNICITY_African', 'PRIMARY_ETHNICITY_African American', 'PRIMARY_ETHNICITY_American', 'PRIMARY_ETHNICITY_Asian Indian', 'PRIMARY_ETHNICITY_Brazilian', 'PRIMARY_ETHNICITY_Cambodian', 'PRIMARY_ETHNICITY_Cape Verdean', 'PRIMARY_ETHNICITY_Caribbean Islander', 'PRIMARY_ETHNICITY_Chinese', 'PRIMARY_ETHNICITY_Colombian', 'PRIMARY_ETHNICITY_Cuban', 'PRIMARY_ETHNICITY_Dominican', 'PRIMARY_ETHNICITY_European', 'PRIMARY_ETHNICITY_Filipino', 'PRIMARY_ETHNICITY_Guatemalan', 'PRIMARY_ETHNICITY_Haitian', 'PRIMARY_ETHNICITY_Honduran', 'PRIMARY_ETHNICITY_Japanese', 'PRIMARY_ETHNICITY_Korean', 'PRIMARY_ETHNICITY_Laotian', 'PRIMARY_ETHNICITY_Mexican, Mexican American, Chicano', 'PRIMARY_ETHNICITY_Middle Eastern', 'PRIMARY_ETHNICITY_Middle Eastern or North African', 'PRIMARY_ETHNICITY_Other', 'PRIMARY_ETHNICITY_Portuguese', 'PRIMARY_ETHNICITY_Puerto Rican', 'PRIMARY_ETHNICITY_Russian', 'PRIMARY_ETHNICITY_Salvadoran', 'PRIMARY_ETHNICITY_Vietnamese', 'D_Insur_at_pull_Other', 'D_Insur_at_pull_Private', 'D_Insur_at_pull_Public', 'F28', 'F20.5', 'F20.2', 'F21', 'F23', 'F24', 'F20.81', 'F25.8', 'F20.1', 'F17.200', 'F20.9', 'F39', 'F25.9', 'F43.10', 'F29', 'F32.A', 'F41.1', 'F41.9', 'F11.20', 'F31.9', 'F22', 'F25.0', 'F20.0', 'F33.1', 'F19.10', 'F10.20', 'F10.10', 'F43.20', 'F14.10', 'F32.9', 'F17.210', 'F14.20', 'F99', 'F25.1', 'F12.10', 'F19.90', 'F90.9', 'F33.9', 'F11.21', 'F33.3', 'F10.21', 'F11.90', 'F12.20', 'F43.21', 'F19.20', 'F20.3', 'F10.11', 'F41.8', 'F41.0', 'F33.2', 'F20.89', 'F34.1']\n"
     ]
    }
   ],
   "source": [
    "continuous_columns = []\n",
    "binary_columns = []\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in df_complete.columns:\n",
    "    unique_values = df_complete[column].dropna().unique()\n",
    "    \n",
    "    # Check if the column is binary (only two unique values, e.g., 0 and 1)\n",
    "    if len(unique_values) == 2:\n",
    "        binary_columns.append(column)\n",
    "    # Check if the column is continuous (numerical and has more than two unique values)\n",
    "    elif pd.api.types.is_numeric_dtype(df_complete[column]) and len(unique_values) > 2:\n",
    "        continuous_columns.append(column)\n",
    "\n",
    "print(\"Continuous Columns:\", continuous_columns)\n",
    "print(\"Binary Columns:\", binary_columns)\n",
    "\n",
    "binary_indices = [df_complete.columns.get_loc(col) for col in binary_columns]\n",
    "continuous_indices = [df_complete.columns.get_loc(col) for col in continuous_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"vvv complete code for mean, mode, knn, missforest, and NAA train and evaluation over all missingness levels for MCAR\n",
    "just copy and paste the above code cell and change the missing mechanism to MNAR and MAR. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GENDER_U': 0.00023781212841854937, 'PRIMARY_RACE_American Indian / Native American': 0.005205732381028882, 'PRIMARY_RACE_Asian Indian': 0.004027419670524987, 'PRIMARY_RACE_Middle Eastern': 0.006146346099236216, 'PRIMARY_RACE_Native Hawaiian / Pacific Islander': 0.0007130969768896847, 'LANGUAGE_Afrikaans': 0.00023781212841854937, 'LANGUAGE_Albanian': 0.0016623090403680998, 'LANGUAGE_American Sign Language': 0.0004755111207151106, 'LANGUAGE_American Sign language & Certified Deaf Interpreter': 0.0004755111207151106, 'LANGUAGE_Amharic / Ethiopia': 0.0016623090403680995, 'LANGUAGE_Arabic': 0.0014251757286814775, 'LANGUAGE_Bassa / Liberia': 0.00023781212841854932, 'LANGUAGE_Bengali / Hindi / Urdu': 0.0, 'LANGUAGE_Bosnian / Croatian / Yulo': 0.0, 'LANGUAGE_Brazilian Portuguese': 0.0007130969768896846, 'LANGUAGE_Chinese / Cantonese': 0.0007130969768896847, 'LANGUAGE_Chinese / Mandarin': 0.0004755111207151106, 'LANGUAGE_French': 0.00047551112071511056, 'LANGUAGE_Fulani / Cameroon': 0.0002378121284185494, 'LANGUAGE_German': 0.0, 'LANGUAGE_Italian': 0.0004755111207151106, 'LANGUAGE_Kirundi / Burundi': 0.0002378121284185494, 'LANGUAGE_Korean': 0.0, 'LANGUAGE_Luganda / Uganda': 0.00023781212841854937, 'LANGUAGE_Nepali': 0.0002378121284185494, 'LANGUAGE_Oromo / Ethiopia': 0.00023781212841854937, 'LANGUAGE_Other': 0.00047551112071511067, 'LANGUAGE_Polish': 0.00023781212841854932, 'LANGUAGE_Portuguese': 0.0021362362553753803, 'LANGUAGE_Romanian': 0.00023781212841854937, 'LANGUAGE_Russian': 0.0, 'LANGUAGE_Somali': 0.0018993292159327343, 'LANGUAGE_Tagalog': 0.00023781212841854937, 'LANGUAGE_Tigrinya': 0.0009505696969422698, 'LANGUAGE_Turkish': 0.00023781212841854937, 'LANGUAGE_Ukrainian': 0.00023781212841854937, 'LANGUAGE_Vietnamese': 0.005205732381028882, 'PRIMARY_ETHNICITY_Asian Indian': 0.0037914177200582422, 'PRIMARY_ETHNICITY_Brazilian': 0.0033190744107587946, 'PRIMARY_ETHNICITY_Cambodian': 0.00023781212841854937, 'PRIMARY_ETHNICITY_Chinese': 0.0026097109258947098, 'PRIMARY_ETHNICITY_Colombian': 0.002846278556971392, 'PRIMARY_ETHNICITY_Cuban': 0.0016623090403680998, 'PRIMARY_ETHNICITY_European': 0.006615974141607957, 'PRIMARY_ETHNICITY_Filipino': 0.00023781212841854937, 'PRIMARY_ETHNICITY_Guatemalan': 0.003319074410758794, 'PRIMARY_ETHNICITY_Honduran': 0.0033190744107587937, 'PRIMARY_ETHNICITY_Japanese': 0.00047551112071511056, 'PRIMARY_ETHNICITY_Korean': 0.0004755111207151106, 'PRIMARY_ETHNICITY_Laotian': 0.00023781212841854937, 'PRIMARY_ETHNICITY_Mexican, Mexican American, Chicano': 0.0011879292808728676, 'PRIMARY_ETHNICITY_Mexican, Mexican American, Chicano/a': 0.0, 'PRIMARY_ETHNICITY_Middle Eastern': 0.006146346099236216, 'PRIMARY_ETHNICITY_Middle Eastern or North African': 0.00023781212841854943, 'PRIMARY_ETHNICITY_Portuguese': 0.0023730301586960384, 'PRIMARY_ETHNICITY_Russian': 0.0007130969768896846, 'PRIMARY_ETHNICITY_Salvadoran': 0.006615974141607957, 'PRIMARY_ETHNICITY_Vietnamese': 0.008957326186147386, 'D_Insur_at_pull_Other': 0.00023781212841854943, 'D_Insur_at_pull_Other Government': 0.0, 'F21': 0.004027419670524987, 'F24': 0.0009505696969422698, 'min_CLOZAPINE': 0.003932989474710871, 'min_OLANZAPINE': 0.009784139353721216, 'min_RISPERIDONE': 0.0030343396623927613, 'max_CLOZAPINE': 0.006801106510472939, 'max_OLANZAPINE': 0.005577709219801989, 'max_RISPERIDONE': 0.0030122872593949103, 'mean_CLOZAPINE': 0.005759388925471429, 'mean_RISPERIDONE': 0.004334837033439388, 'min_BMI': 0.003943293418019411, 'min_HEIGHT': 0.0036877346679453104, 'min_PULSE': 0.007886843430321826, 'min_WEIGHT': 0.003451075690198338, 'max_BMI': 0.00034063285626514397, 'max_HEIGHT': 0.0052257985651912595, 'max_PULSE': 0.0034669708676184087, 'max_WEIGHT': 0.003492632977016909, 'mean_BMI': 0.0002469440480983375, 'mean_WEIGHT': 0.005012681776044592}\n",
      "Number of all features: 163\n",
      "Number of all features with variance > 0.01: 80\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display variance of each feature in df_encoded\n",
    "feature_variance = df_complete.var()\n",
    "high_variance_features = {feature: var for feature, var in feature_variance.items() if var < 0.01}\n",
    "high_variance_features_list = list(high_variance_features.keys())\n",
    "\n",
    "# print(high_variance_features_list)\n",
    "# Function to print entire dataframe\n",
    "def print_full_dataframe(df):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(df)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "print_full_dataframe(high_variance_features)\n",
    "\n",
    "print(f'Number of all features: {len(df_complete.columns.tolist())}')\n",
    "print(f'Number of all features with variance > 0.01: {len(high_variance_features_list)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code for missForest with updated evaluation metric vvvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 🔹 New: Define device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Check if using GPU\n",
    "\n",
    "# Define missingness levels\n",
    "missingness_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# n_repeats = 5  # Number of repetitions\n",
    "n_repeats = 1  # Number of repetitions\n",
    "\n",
    "n_splits = 5  # 5-Fold Cross-validation\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# 🔹 Define NAA Model\n",
    "class NAAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NAAutoencoder, self).__init__()\n",
    "\n",
    "        hidden_dim1 = int(input_dim * 1.5)  # Expand to 1.5x input size\n",
    "        hidden_dim2 = int(input_dim * 2)    # Expand to 2x input size\n",
    "        hidden_dim3 = int(input_dim * 2.5)  # Expand to 2.5x input size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# 🔹 Custom Loss Function: RMSE + BCE\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.sqrt(torch.mean((outputs - targets) ** 2))  # RMSE\n",
    "\n",
    "# 🔹 NAA Training Function (Modified for GPU)\n",
    "def train_naa(train_data, test_data, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)  # Pre-imputation step\n",
    "    train_data_knn = knn_imputer.fit_transform(train_data)\n",
    "    test_data_knn = knn_imputer.transform(test_data)\n",
    "\n",
    "    # 🔹 Move data to GPU\n",
    "    train_tensor = torch.tensor(train_data_knn, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data_knn, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_dim = train_data.shape[1]\n",
    "    model = NAAutoencoder(input_dim).to(device)  # 🔹 Move model to GPU\n",
    "    criterion = RMSELoss().to(device)  # 🔹 Move loss function to GPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            batch_data = batch[0].to(device)  # 🔹 Move batch data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     imputed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Move result back to CPU for numpy processing\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Reconstruct entire dataset\n",
    "\n",
    "    # Create a mask for missing values (NaNs) in the original test data\n",
    "    missing_mask = np.isnan(test_data)  # True where values were missing\n",
    "\n",
    "    # Replace only missing values with imputed values while keeping original non-missing values\n",
    "    imputed_test_data = test_data.copy()\n",
    "    imputed_test_data[missing_mask] = reconstructed_test_data[missing_mask]\n",
    "\n",
    "    return imputed_test_data\n",
    "\n",
    "# 🔹 Define MissForest Imputer\n",
    "def missforest_imputer():\n",
    "    return IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "        max_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 🔹 Start Cross-Validation for Different Missingness Levels\n",
    "for missing_prob in missingness_levels:\n",
    "    results = {\"MissForest\": []}\n",
    "\n",
    "    results_per_n = []\n",
    "    # for _ in tqdm(range(n_repeats)):\n",
    "    for _ in range(n_repeats):\n",
    "        amputer = MultivariateAmputation(patterns=[{\"mechanism\": \"MCAR\", \"prob\": missing_prob, \"incomplete_vars\": high_variance_features_list}])\n",
    "        df_missing = amputer.fit_transform(df_complete)\n",
    "\n",
    "        id_column = \"ID\" if \"ID\" in df_missing.columns else None\n",
    "        df_features = df_missing.drop(columns=[id_column]) if id_column else df_missing\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_index, test_index in kf.split(df_features):\n",
    "            train_df, test_df = df_features.iloc[train_index], df_features.iloc[test_index]\n",
    "            train_complete, test_complete = df_complete.iloc[train_index], df_complete.iloc[test_index]\n",
    "\n",
    "            data_range = test_complete.max() - test_complete.min()\n",
    "            data_range[data_range == 0] = 1  # Avoid division by zero\n",
    "\n",
    "            imputers = {\n",
    "                \"MissForest\": missforest_imputer()\n",
    "            }\n",
    "\n",
    "            \n",
    "\n",
    "            # Initialize dictionaries to store results\n",
    "            rmse_results = {imputer_name: [] for imputer_name in imputers.keys()}\n",
    "            bce_results = {imputer_name: [] for imputer_name in imputers.keys()}\n",
    "            combined_results = {imputer_name: [] for imputer_name in imputers.keys()}\n",
    "\n",
    "\n",
    "\n",
    "            for imputer_name, imputer in imputers.items():\n",
    "                imputed_train = pd.DataFrame(imputer.fit_transform(train_df), columns=train_df.columns)\n",
    "                imputed_test = pd.DataFrame(imputer.transform(test_df), columns=test_df.columns)\n",
    "\n",
    "                # Convert to NumPy arrays\n",
    "                test_complete_np = test_complete.to_numpy()\n",
    "                imputed_test_np = imputed_test.to_numpy()\n",
    "\n",
    "                # Create a boolean mask for missing values\n",
    "                missing_mask = test_df.isnull().to_numpy()\n",
    "\n",
    "                # Evaluate RMSE for Continuous Variables\n",
    "                continuous_rmse = []\n",
    "                for col in continuous_columns:\n",
    "                    col_index = train_df.columns.get_loc(col)\n",
    "                    col_mask = missing_mask[:, col_index]\n",
    "                    # Skip if no imputed values for this column\n",
    "                    if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                    rmse = mean_squared_error(\n",
    "                        test_complete_np[col_mask, col_index],\n",
    "                        imputed_test_np[col_mask, col_index],\n",
    "                        squared=False\n",
    "                    )\n",
    "                    continuous_rmse.append(rmse)\n",
    "                \n",
    "                # Average RMSE across all continuous columns\n",
    "                avg_rmse = np.mean(continuous_rmse)\n",
    "                rmse_results[imputer_name].append(avg_rmse)\n",
    "                \n",
    "                # Evaluate BCE for Binary Variables\n",
    "                binary_bce = []\n",
    "                for col in binary_columns:\n",
    "                    col_index = train_df.columns.get_loc(col)\n",
    "                    col_mask = missing_mask[:, col_index]\n",
    "\n",
    "                    if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Ensure predictions are probabilities between 0 and 1\n",
    "                    pred_probs = np.clip(imputed_test_np[col_mask, col_index], 1e-10, 1 - 1e-10)\n",
    "\n",
    "                    # Checks if there are more than one unique value \n",
    "                    if len(np.unique(test_complete_np[col_mask, col_index])) > 1:\n",
    "                        bce = log_loss(\n",
    "                            test_complete_np[col_mask, col_index],\n",
    "                            pred_probs\n",
    "                        )\n",
    "                        binary_bce.append(bce)\n",
    "                    \n",
    "                # Average BCE across all binary columns\n",
    "                avg_bce = np.mean(binary_bce)\n",
    "                bce_results[imputer_name].append(avg_bce)\n",
    "\n",
    "                # Calculate Combined Metric\n",
    "                combined_metric = avg_rmse + avg_bce\n",
    "                combined_results[imputer_name].append(combined_metric)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Combine and Print Results\n",
    "        \n",
    "        for method in rmse_results:\n",
    "            mean_rmse_per_k_fold = np.mean(rmse_results[method])\n",
    "            std_rmse_per_k_fold = np.std(rmse_results[method])\n",
    "            \n",
    "            mean_bce_per_k_fold = np.mean(bce_results[method])\n",
    "            std_bce_per_k_fold = np.std(bce_results[method])\n",
    "            \n",
    "            mean_combined_per_k_fold = np.mean(combined_results[method])\n",
    "            std_combined_per_k_fold = np.std(combined_results[method])\n",
    "            \n",
    "            results_per_n.append({\n",
    "                \"Missingness\": missing_prob,\n",
    "                \"Method\": method,\n",
    "                \"Mean RMSE\": mean_rmse_per_k_fold,\n",
    "                \"Std RMSE\": std_rmse_per_k_fold,\n",
    "                \"Mean BCE\": mean_bce_per_k_fold,\n",
    "                \"Std BCE\": std_bce_per_k_fold,\n",
    "                \"Mean Combined\": mean_combined_per_k_fold,\n",
    "                \"Std Combined\": std_combined_per_k_fold\n",
    "            })\n",
    "\n",
    "    # Convert the results list to a DataFrame for easier manipulation\n",
    "    df_results = pd.DataFrame(results_per_n)\n",
    "\n",
    "    # Group by Missingness and Method, then calculate the mean and std deviation\n",
    "    averaged_results = df_results.groupby([\"Missingness\", \"Method\"]).agg({\n",
    "        \"Mean RMSE\": [\"mean\", \"std\"],\n",
    "        \"Mean BCE\": [\"mean\", \"std\"],\n",
    "        \"Mean Combined\": [\"mean\", \"std\"]\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate Error Ranges\n",
    "    averaged_results['Error Range RMSE'] = 1.96 * (averaged_results[('Mean RMSE', 'std')] / np.sqrt(n_repeats))\n",
    "    averaged_results['Error Range BCE'] = 1.96 * (averaged_results[('Mean BCE', 'std')] / np.sqrt(n_repeats))\n",
    "    averaged_results['Error Range Combined'] = 1.96 * (averaged_results[('Mean Combined', 'std')] / np.sqrt(n_repeats))\n",
    "\n",
    "    # Adjust to 11 column names\n",
    "    averaged_results.columns = ['Missingness', 'Method', \n",
    "                                'Mean RMSE', 'Std RMSE', 'Error Range RMSE',\n",
    "                                'Mean BCE', 'Std BCE', 'Error Range BCE',\n",
    "                                'Mean Combined', 'Std Combined', 'Error Range Combined']\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Display the averaged results\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Missingness Level: {missing_prob * 100}%\")\n",
    "    print(averaged_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"code that trains mean, mode, knn, naa, and inaa imputations with updated evaluation metrics. run this for MNAR and MAR vvvvvvvv\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on: 0.1% missingness\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 374\u001b[0m\n\u001b[0;32m    371\u001b[0m     combined_results[imputer_name]\u001b[38;5;241m.\u001b[39mappend(combined_metric)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# NAA Imputation\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m imputed_test_naa \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_naa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m imputed_test_naa_np \u001b[38;5;241m=\u001b[39m imputed_test_naa\n\u001b[0;32m    376\u001b[0m missing_mask \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "Cell \u001b[1;32mIn[7], line 230\u001b[0m, in \u001b[0;36mtrain_naa\u001b[1;34m(train_data, test_data, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m    228\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(batch_data)\n\u001b[0;32m    229\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_data)\n\u001b[1;32m--> 230\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m#     imputed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Move result back to CPU for numpy processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\khang\\OneDrive\\Desktop\\SCHOOL\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\khang\\OneDrive\\Desktop\\SCHOOL\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\khang\\OneDrive\\Desktop\\SCHOOL\\NSA\\ACCESS\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress general Python warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Suppress logging warnings from libraries\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# If pyampute is the source of warnings, disable its logger\n",
    "logging.getLogger(\"pyampute\").setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 🔹 New: Define device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Check if using GPU\n",
    "\n",
    "# Define missingness levels\n",
    "missingness_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# n_repeats = 5  # Number of repetitions\n",
    "n_repeats = 5  # Number of repetitions\n",
    "\n",
    "n_splits = 5  # 5-Fold Cross-validation\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# 🔹 I-NAAutoencoder Model with Undercomplete Representation and Dropout\n",
    "class INA_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(INA_Autoencoder, self).__init__()\n",
    "\n",
    "        # Undercomplete Representation: Smaller hidden layers than input\n",
    "        hidden_dim1 = int(input_dim * 0.75)  # Compress to 75% of input size\n",
    "        hidden_dim2 = int(input_dim * 0.60)  # Compress to 60% of input size\n",
    "        hidden_dim3 = int(input_dim * 0.5)   # Compress to 50% of input size\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim1, input_dim),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for both continuous and binary features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "\n",
    "# 🔹 Custom Loss Function: RMSE for Continuous and BCE for Binary\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, binary_indices, continuous_indices):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.binary_indices = binary_indices\n",
    "        self.continuous_indices = continuous_indices\n",
    "        self.bce = nn.BCELoss()  # Binary Cross Entropy for binary features\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Split outputs and targets into binary and continuous parts\n",
    "        binary_outputs = outputs[:, self.binary_indices]\n",
    "        binary_targets = targets[:, self.binary_indices]\n",
    "\n",
    "        continuous_outputs = outputs[:, self.continuous_indices]\n",
    "        continuous_targets = targets[:, self.continuous_indices]\n",
    "\n",
    "        # Calculate BCE for binary features\n",
    "        bce_loss = self.bce(binary_outputs, binary_targets)\n",
    "\n",
    "        # Calculate RMSE for continuous features\n",
    "        rmse_loss = torch.sqrt(torch.mean((continuous_outputs - continuous_targets) ** 2))\n",
    "\n",
    "        # Combine the two losses\n",
    "        total_loss = bce_loss + rmse_loss\n",
    "        return total_loss\n",
    "\n",
    "# 🔹 Training Function for I-NAAutoencoder\n",
    "def train_inaa(train_data, test_data, binary_indices, continuous_indices, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    # 🔹 Random K for KNN Pre-Imputation\n",
    "    k = random.randint(3, 8)\n",
    "    knn_imputer = KNNImputer(n_neighbors=k)\n",
    "    train_data_knn = knn_imputer.fit_transform(train_data)\n",
    "    test_data_knn = knn_imputer.transform(test_data)\n",
    "\n",
    "    # 🔹 Move data to GPU\n",
    "    train_tensor = torch.tensor(train_data_knn, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data_knn, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_dim = train_data.shape[1]\n",
    "    model = INA_Autoencoder(input_dim).to(device)  # 🔹 Move model to GPU\n",
    "    criterion = CustomLoss(binary_indices, continuous_indices).to(device)  # 🔹 Move loss function to GPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch_data = batch[0].to(device)  # 🔹 Move batch data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 🔹 Print epoch loss\n",
    "        # print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Reconstruct entire dataset\n",
    "\n",
    "    # Create a mask for missing values (NaNs) in the original test data\n",
    "    missing_mask = np.isnan(test_data)  # True where values were missing\n",
    "\n",
    "    # Replace only missing values with imputed values while keeping original non-missing values\n",
    "    imputed_test_data = test_data.copy()\n",
    "    imputed_test_data[missing_mask] = reconstructed_test_data[missing_mask]\n",
    "\n",
    "    return imputed_test_data\n",
    "\n",
    "\n",
    "# 🔹 Define NAA Model\n",
    "class NAAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NAAutoencoder, self).__init__()\n",
    "\n",
    "        hidden_dim1 = int(input_dim * 1.5)  # Expand to 1.5x input size\n",
    "        hidden_dim2 = int(input_dim * 2)    # Expand to 2x input size\n",
    "        hidden_dim3 = int(input_dim * 2.5)  # Expand to 2.5x input size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# 🔹 Custom Loss Function: RMSE + BCE\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.sqrt(torch.mean((outputs - targets) ** 2))  # RMSE\n",
    "\n",
    "# 🔹 NAA Training Function (Modified for GPU)\n",
    "def train_naa(train_data, test_data, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)  # Pre-imputation step\n",
    "    train_data_knn = knn_imputer.fit_transform(train_data)\n",
    "    test_data_knn = knn_imputer.transform(test_data)\n",
    "\n",
    "    # 🔹 Move data to GPU\n",
    "    train_tensor = torch.tensor(train_data_knn, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data_knn, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_dim = train_data.shape[1]\n",
    "    model = NAAutoencoder(input_dim).to(device)  # 🔹 Move model to GPU\n",
    "    criterion = RMSELoss().to(device)  # 🔹 Move loss function to GPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            batch_data = batch[0].to(device)  # 🔹 Move batch data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     imputed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Move result back to CPU for numpy processing\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Reconstruct entire dataset\n",
    "\n",
    "    # Create a mask for missing values (NaNs) in the original test data\n",
    "    missing_mask = np.isnan(test_data)  # True where values were missing\n",
    "\n",
    "    # Replace only missing values with imputed values while keeping original non-missing values\n",
    "    imputed_test_data = test_data.copy()\n",
    "    imputed_test_data[missing_mask] = reconstructed_test_data[missing_mask]\n",
    "\n",
    "    return imputed_test_data\n",
    "\n",
    "# 🔹 Define MissForest Imputer\n",
    "def missforest_imputer():\n",
    "    return IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "        max_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 🔹 Start Cross-Validation for Different Missingness Levels\n",
    "for missing_prob in missingness_levels:\n",
    "    print(f'Training on: {missing_prob}% missingness')\n",
    "    # print(\"Training on\", missing_prob)\n",
    "    # nrmse_results = {\"Mean\": [], \"Mode\": [], \"KNN\": [], \"MissForest\": [], \"NAA\": []}\n",
    "    results = {\"Mean\": [], \"Mode\": [], \"KNN\": [], \"NAA\": []}\n",
    "    # rmse_results = {\"MissForest\": []}\n",
    "\n",
    "    results_per_n = []\n",
    "    # for _ in tqdm(range(n_repeats)):\n",
    "    for _ in range(n_repeats):\n",
    "        amputer = MultivariateAmputation(patterns=[{\"mechanism\": \"MNAR\", \"prob\": missing_prob, \"incomplete_vars\": high_variance_features_list}])\n",
    "        df_missing = amputer.fit_transform(df_complete)\n",
    "\n",
    "        id_column = \"ID\" if \"ID\" in df_missing.columns else None\n",
    "        df_features = df_missing.drop(columns=[id_column]) if id_column else df_missing\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_index, test_index in kf.split(df_features):\n",
    "            train_df, test_df = df_features.iloc[train_index], df_features.iloc[test_index]\n",
    "            train_complete, test_complete = df_complete.iloc[train_index], df_complete.iloc[test_index]\n",
    "\n",
    "            data_range = test_complete.max() - test_complete.min()\n",
    "            data_range[data_range == 0] = 1  # Avoid division by zero\n",
    "\n",
    "            imputers = {\n",
    "                \"Mean\": SimpleImputer(strategy=\"mean\"),\n",
    "                \"Mode\": SimpleImputer(strategy=\"most_frequent\"),\n",
    "                \"KNN\": KNNImputer(n_neighbors=5)\n",
    "            }\n",
    "\n",
    "            # Initialize dictionaries to store results\n",
    "            rmse_results = {imputer_name: [] for imputer_name in imputers.keys()}\n",
    "            bce_results = {imputer_name: [] for imputer_name in imputers.keys()}\n",
    "            combined_results = {imputer_name: [] for imputer_name in imputers.keys()}\n",
    "\n",
    "            # Add \"NAA\" to each dictionary\n",
    "            rmse_results[\"NAA\"] = []\n",
    "            bce_results[\"NAA\"] = []\n",
    "            combined_results[\"NAA\"] = []\n",
    "\n",
    "            # Add \"INAA\" to each dictionary\n",
    "            rmse_results[\"INAA\"] = []\n",
    "            bce_results[\"INAA\"] = []\n",
    "            combined_results[\"INAA\"] = []\n",
    "\n",
    "\n",
    "            # imputers = {\n",
    "            #     \"MissForest\": missforest_imputer()\n",
    "            # }\n",
    "\n",
    "\n",
    "            for imputer_name, imputer in imputers.items():\n",
    "                imputed_train = pd.DataFrame(imputer.fit_transform(train_df), columns=train_df.columns)\n",
    "                imputed_test = pd.DataFrame(imputer.transform(test_df), columns=test_df.columns)\n",
    "\n",
    "                # Convert to NumPy arrays\n",
    "                test_complete_np = test_complete.to_numpy()\n",
    "                imputed_test_np = imputed_test.to_numpy()\n",
    "\n",
    "                # Create a boolean mask for missing values\n",
    "                missing_mask = test_df.isnull().to_numpy()\n",
    "\n",
    "                # Evaluate RMSE for Continuous Variables\n",
    "                continuous_rmse = []\n",
    "                for col in continuous_columns:\n",
    "                    col_index = train_df.columns.get_loc(col)\n",
    "                    col_mask = missing_mask[:, col_index]\n",
    "                    # Skip if no imputed values for this column\n",
    "                    if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                    rmse = mean_squared_error(\n",
    "                        test_complete_np[col_mask, col_index],\n",
    "                        imputed_test_np[col_mask, col_index],\n",
    "                        squared=False\n",
    "                    )\n",
    "                    continuous_rmse.append(rmse)\n",
    "                \n",
    "                # Average RMSE across all continuous columns\n",
    "                avg_rmse = np.mean(continuous_rmse)\n",
    "                rmse_results[imputer_name].append(avg_rmse)\n",
    "                \n",
    "                # Evaluate BCE for Binary Variables\n",
    "                binary_bce = []\n",
    "                for col in binary_columns:\n",
    "                    col_index = train_df.columns.get_loc(col)\n",
    "                    col_mask = missing_mask[:, col_index]\n",
    "\n",
    "                    if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Ensure predictions are probabilities between 0 and 1\n",
    "                    pred_probs = np.clip(imputed_test_np[col_mask, col_index], 1e-10, 1 - 1e-10)\n",
    "\n",
    "                    # Checks if there are more than one unique value \n",
    "                    if len(np.unique(test_complete_np[col_mask, col_index])) > 1:\n",
    "                        bce = log_loss(\n",
    "                            test_complete_np[col_mask, col_index],\n",
    "                            pred_probs\n",
    "                        )\n",
    "                        binary_bce.append(bce)\n",
    "                    \n",
    "                # Average BCE across all binary columns\n",
    "                avg_bce = np.mean(binary_bce)\n",
    "                bce_results[imputer_name].append(avg_bce)\n",
    "\n",
    "                # Calculate Combined Metric\n",
    "                combined_metric = avg_rmse + avg_bce\n",
    "                combined_results[imputer_name].append(combined_metric)\n",
    "\n",
    "            # NAA Imputation\n",
    "            imputed_test_naa = train_naa(train_df.values, test_df.values)\n",
    "            imputed_test_naa_np = imputed_test_naa\n",
    "            missing_mask = test_df.isnull().to_numpy()\n",
    "\n",
    "            # INAA Imputation\n",
    "            imputed_test_inaa = train_inaa(train_df.values, test_df.values, binary_indices, continuous_indices)\n",
    "            imputed_test_inaa_np = imputed_test_inaa\n",
    "            missing_mask = test_df.isnull().to_numpy()\n",
    "\n",
    "            # INAA and NAA RMSE for Continuous Variables\n",
    "            continuous_rmse_naa = []\n",
    "            continuous_rmse_inaa = []\n",
    "\n",
    "            for col in continuous_columns:\n",
    "                col_index = train_df.columns.get_loc(col)\n",
    "                col_mask = missing_mask[:, col_index]\n",
    "                if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                rmse_naa = mean_squared_error(\n",
    "                    test_complete_np[col_mask, col_index],\n",
    "                    imputed_test_naa_np[col_mask, col_index],\n",
    "                    squared=False\n",
    "                )\n",
    "                rmse_inaa = mean_squared_error(\n",
    "                    test_complete_np[col_mask, col_index],\n",
    "                    imputed_test_inaa_np[col_mask, col_index],\n",
    "                    squared=False\n",
    "                )\n",
    "         \n",
    "                continuous_rmse_naa.append(rmse_naa)\n",
    "                continuous_rmse_inaa.append(rmse_inaa)\n",
    "\n",
    "            avg_rmse_naa = np.mean(continuous_rmse_naa)\n",
    "            avg_rmse_inaa = np.mean(continuous_rmse_inaa)\n",
    "            rmse_results[\"NAA\"].append(avg_rmse_naa)\n",
    "            rmse_results[\"INAA\"].append(avg_rmse_inaa)\n",
    "\n",
    "            # NAA BCE for Binary Variables\n",
    "            binary_bce_naa = []\n",
    "            binary_bce_inaa = []\n",
    "            for col in binary_columns:\n",
    "                col_index = train_df.columns.get_loc(col)\n",
    "                col_mask = missing_mask[:, col_index]\n",
    "\n",
    "                if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                \n",
    "                pred_probs_naa = np.clip(imputed_test_naa_np[col_mask, col_index], 1e-10, 1 - 1e-10)\n",
    "                pred_probs_inaa = np.clip(imputed_test_inaa_np[col_mask, col_index], 1e-10, 1 - 1e-10)\n",
    "                \n",
    "                if len(np.unique(test_complete_np[col_mask, col_index])) > 1:\n",
    "                    bce_naa = log_loss(\n",
    "                        test_complete_np[col_mask, col_index],\n",
    "                        pred_probs_naa\n",
    "                    )\n",
    "                    binary_bce_naa.append(bce_naa)\n",
    "                    bce_inaa = log_loss(\n",
    "                        test_complete_np[col_mask, col_index],\n",
    "                        pred_probs_inaa\n",
    "                    )\n",
    "                    binary_bce_inaa.append(bce_inaa)\n",
    "\n",
    "            avg_bce_naa = np.mean(binary_bce_naa)\n",
    "            bce_results[\"NAA\"].append(avg_bce_naa)\n",
    "            avg_bce_inaa = np.mean(binary_bce_inaa)\n",
    "            bce_results[\"INAA\"].append(avg_bce_inaa)\n",
    "\n",
    "            # Calculate Combined Metric for INAA and NAA\n",
    "            combined_metric_naa = avg_rmse_naa + avg_bce_naa\n",
    "            combined_results[\"NAA\"].append(combined_metric_naa)\n",
    "            combined_metric_inaa = avg_rmse_inaa + avg_bce_inaa\n",
    "            combined_results[\"INAA\"].append(combined_metric_inaa)\n",
    "\n",
    "        # Combine and Print Results\n",
    "        \n",
    "        for method in rmse_results:\n",
    "            mean_rmse_per_k_fold = np.mean(rmse_results[method])\n",
    "            std_rmse_per_k_fold = np.std(rmse_results[method])\n",
    "            \n",
    "            mean_bce_per_k_fold = np.mean(bce_results[method])\n",
    "            std_bce_per_k_fold = np.std(bce_results[method])\n",
    "            \n",
    "            mean_combined_per_k_fold = np.mean(combined_results[method])\n",
    "            std_combined_per_k_fold = np.std(combined_results[method])\n",
    "            \n",
    "            results_per_n.append({\n",
    "                \"Missingness\": missing_prob,\n",
    "                \"Method\": method,\n",
    "                \"Mean RMSE\": mean_rmse_per_k_fold,\n",
    "                \"Std RMSE\": std_rmse_per_k_fold,\n",
    "                \"Mean BCE\": mean_bce_per_k_fold,\n",
    "                \"Std BCE\": std_bce_per_k_fold,\n",
    "                \"Mean Combined\": mean_combined_per_k_fold,\n",
    "                \"Std Combined\": std_combined_per_k_fold\n",
    "            })\n",
    "\n",
    "    # Convert the results list to a DataFrame for easier manipulation\n",
    "    df_results = pd.DataFrame(results_per_n)\n",
    "\n",
    "    # Group by Missingness and Method, then calculate the mean and std deviation\n",
    "    averaged_results = df_results.groupby([\"Missingness\", \"Method\"]).agg({\n",
    "        \"Mean RMSE\": [\"mean\", \"std\"],\n",
    "        \"Mean BCE\": [\"mean\", \"std\"],\n",
    "        \"Mean Combined\": [\"mean\", \"std\"]\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate Error Ranges\n",
    "    averaged_results['Error Range RMSE'] = 1.96 * (averaged_results[('Mean RMSE', 'std')] / np.sqrt(n_repeats))\n",
    "    averaged_results['Error Range BCE'] = 1.96 * (averaged_results[('Mean BCE', 'std')] / np.sqrt(n_repeats))\n",
    "    averaged_results['Error Range Combined'] = 1.96 * (averaged_results[('Mean Combined', 'std')] / np.sqrt(n_repeats))\n",
    "\n",
    "    # Adjust to 11 column names\n",
    "    averaged_results.columns = ['Missingness', 'Method', \n",
    "                                'Mean RMSE', 'Std RMSE', 'Error Range RMSE',\n",
    "                                'Mean BCE', 'Std BCE', 'Error Range BCE',\n",
    "                                'Mean Combined', 'Std Combined', 'Error Range Combined']\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Display the averaged results\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Missingness Level: {missing_prob * 100}%\")\n",
    "    print(averaged_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code for mean, mode, knn, naa imputation with updated evaluation metric ^^^^ (tomorrow change a bit to just run missForest)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN AND INFERENCE ON I-NAA MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on: 10.0%\n",
      "✅ Best I-NAA model for 10% missingness saved at: best_inaa_models\\best_INAA_MAR_10.pth\n",
      "Training on: 20.0%\n",
      "✅ Best I-NAA model for 20% missingness saved at: best_inaa_models\\best_INAA_MAR_20.pth\n",
      "Training on: 30.0%\n",
      "✅ Best I-NAA model for 30% missingness saved at: best_inaa_models\\best_INAA_MAR_30.pth\n",
      "\n",
      "📌 Summary of saved best I-NAA models per missingness level:\n",
      "- 10% missingness: best_inaa_models\\best_INAA_MAR_10.pth\n",
      "- 20% missingness: best_inaa_models\\best_INAA_MAR_20.pth\n",
      "- 30% missingness: best_inaa_models\\best_INAA_MAR_30.pth\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 🔹 New: Define device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Check if using GPU\n",
    "\n",
    "# Define missingness levels\n",
    "# missingness_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "missingness_levels = [0.1, 0.2, 0.3]\n",
    "\n",
    "# n_repeats = 5  # Number of repetitions\n",
    "n_repeats = 5  # Number of repetitions\n",
    "\n",
    "n_splits = 5  # 5-Fold Cross-validation\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# 🔹 I-NAAutoencoder Model with Undercomplete Representation and Dropout\n",
    "class INA_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(INA_Autoencoder, self).__init__()\n",
    "\n",
    "        # Undercomplete Representation: Smaller hidden layers than input\n",
    "        hidden_dim1 = int(input_dim * 0.75)  # Compress to 75% of input size\n",
    "        hidden_dim2 = int(input_dim * 0.60)  # Compress to 60% of input size\n",
    "        hidden_dim3 = int(input_dim * 0.5)   # Compress to 50% of input size\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim3, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim1, input_dim),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for both continuous and binary features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "\n",
    "# 🔹 Custom Loss Function: RMSE for Continuous and BCE for Binary\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, binary_indices, continuous_indices):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.binary_indices = binary_indices\n",
    "        self.continuous_indices = continuous_indices\n",
    "        self.bce = nn.BCELoss()  # Binary Cross Entropy for binary features\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Split outputs and targets into binary and continuous parts\n",
    "        binary_outputs = outputs[:, self.binary_indices]\n",
    "        binary_targets = targets[:, self.binary_indices]\n",
    "\n",
    "        continuous_outputs = outputs[:, self.continuous_indices]\n",
    "        continuous_targets = targets[:, self.continuous_indices]\n",
    "\n",
    "        # Calculate BCE for binary features\n",
    "        bce_loss = self.bce(binary_outputs, binary_targets)\n",
    "\n",
    "        # Calculate RMSE for continuous features\n",
    "        rmse_loss = torch.sqrt(torch.mean((continuous_outputs - continuous_targets) ** 2))\n",
    "\n",
    "        # Combine the two losses\n",
    "        total_loss = bce_loss + rmse_loss\n",
    "        return total_loss\n",
    "\n",
    "# 🔹 Training Function for I-NAAutoencoder\n",
    "def train_inaa(train_data, test_data, binary_indices, continuous_indices, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    # 🔹 Random K for KNN Pre-Imputation\n",
    "    k = random.randint(3, 8)\n",
    "    knn_imputer = KNNImputer(n_neighbors=k)\n",
    "    train_data_knn = knn_imputer.fit_transform(train_data)\n",
    "    test_data_knn = knn_imputer.transform(test_data)\n",
    "\n",
    "    # 🔹 Move data to GPU\n",
    "    train_tensor = torch.tensor(train_data_knn, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data_knn, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    input_dim = train_data.shape[1]\n",
    "    model = INA_Autoencoder(input_dim).to(device)  # 🔹 Move model to GPU\n",
    "    criterion = CustomLoss(binary_indices, continuous_indices).to(device)  # 🔹 Move loss function to GPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch_data = batch[0].to(device)  # 🔹 Move batch data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 🔹 Print epoch loss\n",
    "        # print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_test_data = model(test_tensor).cpu().numpy()  # 🔹 Reconstruct entire dataset\n",
    "\n",
    "    # Create a mask for missing values (NaNs) in the original test data\n",
    "    missing_mask = np.isnan(test_data)  # True where values were missing\n",
    "\n",
    "    # Replace only missing values with imputed values while keeping original non-missing values\n",
    "    imputed_test_data = test_data.copy()\n",
    "    imputed_test_data[missing_mask] = reconstructed_test_data[missing_mask]\n",
    "\n",
    "    return imputed_test_data\n",
    "\n",
    "# 🔹 Custom Loss Function: RMSE + BCE\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.sqrt(torch.mean((outputs - targets) ** 2))  # RMSE\n",
    "\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = \"best_inaa_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to track the best model per missingness level\n",
    "best_models = {}\n",
    "\n",
    "# 🔹 Start Cross-Validation for Different Missingness Levels\n",
    "for missing_prob in missingness_levels:\n",
    "    print(f'Training on: {missing_prob * 100}%')\n",
    "    best_combined_score = float('inf')  # Initialize to a large number\n",
    "    best_model_state = None\n",
    "\n",
    "    rmse_results = {}\n",
    "    results_per_n = []\n",
    "    # for _ in tqdm(range(n_repeats)):\n",
    "    for _ in range(n_repeats):\n",
    "        amputer = MultivariateAmputation(patterns=[{\"mechanism\": \"MAR\", \"prob\": missing_prob, \"incomplete_vars\": high_variance_features_list}])\n",
    "        df_missing = amputer.fit_transform(df_complete)\n",
    "\n",
    "        id_column = \"ID\" if \"ID\" in df_missing.columns else None\n",
    "        df_features = df_missing.drop(columns=[id_column]) if id_column else df_missing\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_index, test_index in kf.split(df_features):\n",
    "            train_df, test_df = df_features.iloc[train_index], df_features.iloc[test_index]\n",
    "            train_complete, test_complete = df_complete.iloc[train_index], df_complete.iloc[test_index]\n",
    "\n",
    "            data_range = test_complete.max() - test_complete.min()\n",
    "            data_range[data_range == 0] = 1  # Avoid division by zero\n",
    "\n",
    "            # Add \"INAA\" to each dictionary\n",
    "            rmse_results[\"INAA\"] = []\n",
    "            bce_results[\"INAA\"] = []\n",
    "            combined_results[\"INAA\"] = []\n",
    "\n",
    "\n",
    "            # Train INAA\n",
    "            model = INA_Autoencoder(train_df.shape[1]).to(device)  # Initialize model\n",
    "            imputed_test_inaa = train_inaa(train_df.values, test_df.values, binary_indices, continuous_indices)\n",
    "            imputed_test_inaa_np = imputed_test_inaa\n",
    "            missing_mask = test_df.isnull().to_numpy()\n",
    "\n",
    "            # INAA and NAA RMSE for Continuous Variables\n",
    "            continuous_rmse_inaa = []\n",
    "\n",
    "            for col in continuous_columns:\n",
    "                col_index = train_df.columns.get_loc(col)\n",
    "                col_mask = missing_mask[:, col_index]\n",
    "                if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "\n",
    "                rmse_inaa = mean_squared_error(\n",
    "                    test_complete_np[col_mask, col_index],\n",
    "                    imputed_test_inaa_np[col_mask, col_index],\n",
    "                    squared=False\n",
    "                )\n",
    "         \n",
    "                continuous_rmse_inaa.append(rmse_inaa)\n",
    "\n",
    "            avg_rmse_inaa = np.mean(continuous_rmse_inaa)\n",
    "            rmse_results[\"INAA\"].append(avg_rmse_inaa)\n",
    "\n",
    "            # NAA BCE for Binary Variables\n",
    "            binary_bce_inaa = []\n",
    "            for col in binary_columns:\n",
    "                col_index = train_df.columns.get_loc(col)\n",
    "                col_mask = missing_mask[:, col_index]\n",
    "\n",
    "                if np.sum(col_mask) == 0:\n",
    "                        continue\n",
    "                \n",
    "                pred_probs_inaa = np.clip(imputed_test_inaa_np[col_mask, col_index], 1e-10, 1 - 1e-10)\n",
    "                \n",
    "                if len(np.unique(test_complete_np[col_mask, col_index])) > 1:\n",
    "                    bce_inaa = log_loss(\n",
    "                        test_complete_np[col_mask, col_index],\n",
    "                        pred_probs_inaa\n",
    "                    )\n",
    "                    binary_bce_inaa.append(bce_inaa)\n",
    "\n",
    "            avg_bce_inaa = np.mean(binary_bce_inaa)\n",
    "            bce_results[\"INAA\"].append(avg_bce_inaa)\n",
    "\n",
    "            # Calculate Combined Metric for INAA\n",
    "            combined_metric_inaa = np.sqrt((avg_rmse_inaa * avg_rmse_inaa) + (avg_bce_inaa * avg_bce_inaa))\n",
    "            combined_results[\"INAA\"].append(combined_metric_inaa)\n",
    "\n",
    "            if combined_metric_inaa < best_combined_score:\n",
    "                best_combined_score = combined_metric_inaa\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    # Save the best model for the missingness level\n",
    "    if best_model_state:\n",
    "        model_path = os.path.join(model_save_dir, f\"best_INAA_MAR_{int(missing_prob*100)}.pth\")\n",
    "        torch.save(best_model_state, model_path)\n",
    "        best_models[missing_prob] = model_path\n",
    "        print(f\"✅ Best I-NAA model for {int(missing_prob*100)}% missingness saved at: {model_path}\")   \n",
    "        # Combine and Print Results\n",
    "        \n",
    "        for method in rmse_results:\n",
    "            mean_rmse_per_k_fold = np.mean(rmse_results[method])\n",
    "            std_rmse_per_k_fold = np.std(rmse_results[method])\n",
    "            \n",
    "            mean_bce_per_k_fold = np.mean(bce_results[method])\n",
    "            std_bce_per_k_fold = np.std(bce_results[method])\n",
    "            \n",
    "            mean_combined_per_k_fold = np.mean(combined_results[method])\n",
    "            std_combined_per_k_fold = np.std(combined_results[method])\n",
    "            \n",
    "            results_per_n.append({\n",
    "                \"Missingness\": missing_prob,\n",
    "                \"Method\": method,\n",
    "                \"Mean RMSE\": mean_rmse_per_k_fold,\n",
    "                \"Std RMSE\": std_rmse_per_k_fold,\n",
    "                \"Mean BCE\": mean_bce_per_k_fold,\n",
    "                \"Std BCE\": std_bce_per_k_fold,\n",
    "                \"Mean Combined\": mean_combined_per_k_fold,\n",
    "                \"Std Combined\": std_combined_per_k_fold\n",
    "            })\n",
    "\n",
    "    # Convert the results list to a DataFrame for easier manipulation\n",
    "    df_results = pd.DataFrame(results_per_n)\n",
    "\n",
    "    # Group by Missingness and Method, then calculate the mean and std deviation\n",
    "    averaged_results = df_results.groupby([\"Missingness\", \"Method\"]).agg({\n",
    "        \"Mean RMSE\": [\"mean\", \"std\"],\n",
    "        \"Mean BCE\": [\"mean\", \"std\"],\n",
    "        \"Mean Combined\": [\"mean\", \"std\"]\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate Error Ranges\n",
    "    averaged_results['Error Range RMSE'] = 1.96 * (averaged_results[('Mean RMSE', 'std')] / np.sqrt(n_repeats))\n",
    "    averaged_results['Error Range BCE'] = 1.96 * (averaged_results[('Mean BCE', 'std')] / np.sqrt(n_repeats))\n",
    "    averaged_results['Error Range Combined'] = 1.96 * (averaged_results[('Mean Combined', 'std')] / np.sqrt(n_repeats))\n",
    "\n",
    "    # Adjust to 11 column names\n",
    "    averaged_results.columns = ['Missingness', 'Method', \n",
    "                                'Mean RMSE', 'Std RMSE', 'Error Range RMSE',\n",
    "                                'Mean BCE', 'Std BCE', 'Error Range BCE',\n",
    "                                'Mean Combined', 'Std Combined', 'Error Range Combined']\n",
    "\n",
    "\n",
    "# Print summary of saved models\n",
    "print(\"\\n📌 Summary of saved best I-NAA models per missingness level:\")\n",
    "for missing_prob, path in best_models.items():\n",
    "    print(f\"- {int(missing_prob*100)}% missingness: {path}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
