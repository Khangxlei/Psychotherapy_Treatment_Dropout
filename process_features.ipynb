{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_labs[\"RESULT_VALUE_NUM\"] = pd.to_numeric(filtered_labs[\"RESULT_VALUE_NUM\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  demo_age  RPL_THEME1  GENDER_F  GENDER_M  GENDER_U  \\\n",
      "0  10000        49        0.49      True     False     False   \n",
      "1  10001        67        0.75      True     False     False   \n",
      "2  10002        80        0.79      True     False     False   \n",
      "3  10004        83        0.55      True     False     False   \n",
      "4  10005        82        0.66     False      True     False   \n",
      "\n",
      "  PRIMARY_RACE_American Indian / Native American PRIMARY_RACE_Asian  \\\n",
      "0                                          False              False   \n",
      "1                                          False              False   \n",
      "2                                          False              False   \n",
      "3                                          False              False   \n",
      "4                                          False              False   \n",
      "\n",
      "  PRIMARY_RACE_Asian Indian PRIMARY_RACE_Black / African American  ...  \\\n",
      "0                     False                                     1  ...   \n",
      "1                     False                                 False  ...   \n",
      "2                     False                                     1  ...   \n",
      "3                     False                                  True  ...   \n",
      "4                     False                                     1  ...   \n",
      "\n",
      "   F12.20 F43.21 F19.20  F20.3  F10.11 F41.8  F41.0  F33.2  F20.89  F34.1  \n",
      "0       0      0      0      0       0     0      0      0       0      0  \n",
      "1       0      0      0      0       0     0      0      0       0      0  \n",
      "2       0      0      0      0       0     0      0      0       0      0  \n",
      "3       0      0      0      0       0     0      0      0       0      0  \n",
      "4       0      0      0      0       0     0      0      0       0      0  \n",
      "\n",
      "[5 rows x 168 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:333: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['PRIMARY_RACE_Chinese'] == 1, 'PRIMARY_RACE_Asian'] = 1\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:336: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['PRIMARY_RACE_Native Hawaiian / Other Pacific Islander'] == 1, 'PRIMARY_RACE_Native Hawaiian / Pacific Islander'] = 1\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:340: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['PRIMARY_RACE_American Indian or Alaskan Native'] == 1, 'PRIMARY_RACE_American Indian / Native American'] = 1\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:343: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['PRIMARY_ETHNICITY_Middle Eastern'] == 1, 'PRIMARY_RACE_Middle Eastern'] = 1\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:346: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[['PRIMARY_ETHNICITY_Russian', 'PRIMARY_ETHNICITY_American', 'PRIMARY_ETHNICITY_European']].sum(axis=1) > 0, 'PRIMARY_RACE_White'] = 1\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:349: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['PRIMARY_ETHNICITY_Asian Indian'] == 1, 'PRIMARY_RACE_Asian Indian'] = 1\n",
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\4107370961.py:352: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[['PRIMARY_ETHNICITY_African', 'PRIMARY_ETHNICITY_African American', 'PRIMARY_ETHNICITY_Cape Verdean', 'PRIMARY_ETHNICITY_Caribbean Islander', 'PRIMARY_ETHNICITY_Haitian', 'PRIMARY_ETHNICITY_Middle Eastern or North African']].sum(axis=1) > 0, 'PRIMARY_RACE_Black / African American'] = 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import numpy as np\n",
    "\n",
    "# Define the file name (ensure the file is in your working directory or provide the full path)\n",
    "file_name = \"data/S001_u7.5.2024_DI_from_3.2024_Merged_Demo_Geo_by_Tithi.xlsx\"\n",
    "\n",
    "# Specify the columns to be read from the Excel file\n",
    "columns_to_use = [\n",
    "    'ID', \n",
    "    'GENDER', \n",
    "    'demo_age', \n",
    "    'PRIMARY_RACE', \n",
    "    'LANGUAGE', \n",
    "    'PRIMARY_ETHNICITY', \n",
    "    'D_Insur_at_pull', \n",
    "    'RPL_THEME1'\n",
    "]\n",
    "\n",
    "# Read the Excel file using pandas; if the file has multiple sheets, you may need to specify the sheet_name parameter\n",
    "df = pd.read_excel(file_name, usecols=columns_to_use)\n",
    "\n",
    "\n",
    "df['PRIMARY_RACE'] = df['PRIMARY_RACE'].replace(\n",
    "    [\"Declined / Not Available\", \"Choose not to Answer\", \"Unknown\", ''],\n",
    "    'Unknown'\n",
    ")\n",
    "\n",
    "# Replace NaN values with \"unknown\"\n",
    "df['LANGUAGE'] = df['LANGUAGE'].fillna('Unknown')\n",
    "\n",
    "# Replace blank strings (including those that are only whitespace) with \"unknown\"\n",
    "df['LANGUAGE'] = df['LANGUAGE'].replace(r'^\\s*$', 'Unknown', regex=True)\n",
    "\n",
    "df['PRIMARY_ETHNICITY'] = df['PRIMARY_ETHNICITY'].replace(\n",
    "    [\"Patient Refused\", \"Patient chooses not to answer\", \"Unknown/Not Specified\",\"\"],\n",
    "    'Unknown'\n",
    ")\n",
    "\n",
    "# Replace NaN values with \"unknown\"\n",
    "df['D_Insur_at_pull'] = df['D_Insur_at_pull'].fillna('Unknown')\n",
    "\n",
    "# Replace blank strings (including those that are only whitespace) with \"unknown\"\n",
    "df['D_Insur_at_pull'] = df['D_Insur_at_pull'].replace(r'^\\s*$', 'Unknown', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define which columns are categorical and need one hot encoding\n",
    "categorical_columns = ['GENDER', 'PRIMARY_RACE', 'LANGUAGE', 'PRIMARY_ETHNICITY', 'D_Insur_at_pull']\n",
    "\n",
    "\n",
    "# # Apply one hot encoding to the categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "\"\"\"Replaces negative values, values > 1, and non-numeric values in 'RPL_THEME1' with NaN.\"\"\"\n",
    "df_encoded[\"RPL_THEME1\"] = pd.to_numeric(df_encoded[\"RPL_THEME1\"], errors=\"coerce\")  # Convert non-numeric to NaN\n",
    "df_encoded.loc[(df_encoded[\"RPL_THEME1\"] < 0) | (df_encoded[\"RPL_THEME1\"] > 1), \"RPL_THEME1\"] = np.nan  # Replace invalid values\n",
    "\n",
    "\n",
    "# Load the Excel spreadsheet\n",
    "data_path = \"data/anonymized_H-43413 Qualifying Encounters.xlsx\"\n",
    "df_icd10 = pd.read_excel(data_path)\n",
    "\n",
    "# List of ICD-10 codes to check\n",
    "# icd10_codes = [\n",
    "#     \"F20.9\", \"F25.0\", \"F25.9\", \"F22\", \"F20.3\", \"F29\", \"F20.89\", \"F20.0\", \"F25.8\", \"F23\", \n",
    "#     \"F20.2\", \"F20.1\", \"F25.1\", \"F20.5\", \"F28\", \"F21\", \"F20.81\", \"F24\"\n",
    "# ]\n",
    "\n",
    "icd10_codes = ['F28', 'F20.5', 'F20.2', 'F21', 'F23', 'F24', 'F20.81', 'F25.8', 'F20.1']\n",
    "\n",
    "# Create binary columns for each ICD-10 code\n",
    "df_icd10_binary = df_icd10.pivot_table(index='ID', columns='ICD-10', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Ensure all specified ICD-10 codes are present in the dataframe\n",
    "df_icd10_binary = df_icd10_binary.reindex(columns=icd10_codes, fill_value=0)\n",
    "\n",
    "# Reset index to bring ID back as a column\n",
    "df_icd10_binary.reset_index(inplace=True)\n",
    "\n",
    "# Merge with df_encoded\n",
    "df_encoded = df_encoded.merge(df_icd10_binary, on=\"ID\", how=\"left\")\n",
    "\n",
    "# Fill NaN values with 0 (for IDs that did not appear in the ICD-10 dataset)\n",
    "df_encoded.fillna(0, inplace=True)\n",
    "\n",
    "# Convert to binary: if value is 1 or more, set to 1, otherwise keep 0\n",
    "df_encoded[icd10_codes] = (df_encoded[icd10_codes] > 0).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"data/anonymized_H-43413 Amb Med Data.xlsx\"\n",
    "df_med = pd.read_excel(file_path)\n",
    "\n",
    "# Define medication keywords (partial match allowed)\n",
    "medications = [\"CLOZAPINE\", \"RISPERIDONE\", \"OLANZAPINE\"]\n",
    "\n",
    "# Standardize medication names (uppercase for uniformity)\n",
    "df_med[\"MEDICATION NAME\"] = df_med[\"MEDICATION NAME\"].str.upper()\n",
    "\n",
    "# Function to check if any medication keyword is in the \"MEDICATION NAME\"\n",
    "def contains_medication(med_name, target_list):\n",
    "    return any(target in med_name for target in target_list)\n",
    "\n",
    "# Filter only rows where \"MEDICATION NAME\" contains any target medication\n",
    "df_filtered = df_med[df_med[\"MEDICATION NAME\"].apply(lambda x: contains_medication(x, medications))].copy()\n",
    "\n",
    "# Function to extract numeric dose values from strings like \"10 MG\"\n",
    "def extract_numeric_dose(dose):\n",
    "    if isinstance(dose, str):\n",
    "        match = re.search(r\"[\\d]+(?:\\.\\d+)?\", dose)  # Extract number including decimals\n",
    "        return float(match.group()) if match else None\n",
    "    return dose  # If already a number, return as is\n",
    "\n",
    "# Apply extraction function to clean \"DOSE\" column\n",
    "df_filtered[\"DOSE\"] = df_filtered[\"DOSE\"].apply(extract_numeric_dose)\n",
    "\n",
    "# Drop rows where DOSE couldn't be extracted\n",
    "df_filtered = df_filtered.dropna(subset=[\"DOSE\"])\n",
    "\n",
    "# Convert DOSE to numeric\n",
    "df_filtered[\"DOSE\"] = df_filtered[\"DOSE\"].astype(float)\n",
    "\n",
    "# Map full medication names back to their generic form (CLOZAPINE, RISPERIDONE, OLANZAPINE)\n",
    "def get_medication_generic_name(med_name):\n",
    "    for med in medications:\n",
    "        if med in med_name:\n",
    "            return med\n",
    "    return None\n",
    "\n",
    "df_filtered[\"GENERIC_MED_NAME\"] = df_filtered[\"MEDICATION NAME\"].apply(get_medication_generic_name)\n",
    "\n",
    "# Aggregate statistics per ID and Medication\n",
    "df_stats = df_filtered.groupby([\"ID\", \"GENERIC_MED_NAME\"])[\"DOSE\"].agg([\"min\", \"max\", \"mean\"]).reset_index()\n",
    "\n",
    "# Pivot table to create separate columns for each medication and statistic\n",
    "df_pivot = df_stats.pivot(index=\"ID\", columns=\"GENERIC_MED_NAME\", values=[\"min\", \"max\", \"mean\"])\n",
    "\n",
    "# Rename columns for clarity (e.g., min_CLOZAPINE, max_CLOZAPINE, mean_CLOZAPINE)\n",
    "df_pivot.columns = [f\"{stat}_{med}\" for stat, med in df_pivot.columns]\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Ensure all medication dosage variables exist in df_encoded, even if missing in df_pivot\n",
    "for med in medications:\n",
    "    for stat in [\"min\", \"max\", \"mean\"]:\n",
    "        col_name = f\"{stat}_{med}\"\n",
    "        if col_name not in df_pivot.columns:\n",
    "            df_pivot[col_name] = 0  # Add missing columns with 0\n",
    "\n",
    "# Merge with df_encoded\n",
    "df_encoded = df_encoded.merge(df_pivot, on=\"ID\", how=\"left\")\n",
    "\n",
    "# Replace NaN values in the 9 medication-related columns with 0\n",
    "for med in medications:\n",
    "    for stat in [\"min\", \"max\", \"mean\"]:\n",
    "        col_name = f\"{stat}_{med}\"\n",
    "        df_encoded[col_name] = df_encoded[col_name].fillna(0)\n",
    "\n",
    "# Load the vitals CSV file\n",
    "vitals_file_path = \"data/anonymized_H43413_vitals.csv\"\n",
    "df_vitals = pd.read_csv(vitals_file_path)\n",
    "\n",
    "# Standardize FLO_DISPLAY_NM (uppercase for uniformity)\n",
    "df_vitals[\"FLO_DISPLAY_NM\"] = df_vitals[\"FLO_DISPLAY_NM\"].str.upper()\n",
    "\n",
    "# Define relevant vitals (case-insensitive search)\n",
    "vitals = [\"HEIGHT\", \"WEIGHT\", \"BMI\", \"PULSE\"]\n",
    "\n",
    "# Function to check if any vital keyword is in \"FLO_DISPLAY_NM\"\n",
    "def contains_vital(vital_name, target_list):\n",
    "    return any(target in vital_name for target in target_list)\n",
    "\n",
    "# Filter only rows where \"FLO_DISPLAY_NM\" contains relevant vitals\n",
    "df_filtered = df_vitals[df_vitals[\"FLO_DISPLAY_NM\"].apply(lambda x: contains_vital(x, vitals))].copy()\n",
    "\n",
    "# Function to extract numeric values from vitals data\n",
    "def extract_numeric_value(value):\n",
    "    if isinstance(value, str):\n",
    "        match = re.search(r\"[\\d]+(?:\\.\\d+)?\", value)  # Extract number including decimals\n",
    "        return float(match.group()) if match else None\n",
    "    return value  # If already a number, return as is\n",
    "\n",
    "# Apply extraction function to clean values\n",
    "df_filtered[\"ENTERED_VALUE\"] = df_filtered[\"ENTERED_VALUE\"].apply(extract_numeric_value)\n",
    "\n",
    "# Drop rows where VALUE couldn't be extracted\n",
    "df_filtered = df_filtered.dropna(subset=[\"ENTERED_VALUE\"])\n",
    "\n",
    "# Convert VALUE to numeric\n",
    "df_filtered[\"ENTERED_VALUE\"] = df_filtered[\"ENTERED_VALUE\"].astype(float)\n",
    "\n",
    "# Map full FLO_DISPLAY_NM names back to generic names (HEIGHT, WEIGHT, BMI)\n",
    "def get_vital_name(display_name):\n",
    "    for vital in vitals:\n",
    "        if vital in display_name:\n",
    "            return vital\n",
    "    return None\n",
    "\n",
    "df_filtered[\"VITAL_NAME\"] = df_filtered[\"FLO_DISPLAY_NM\"].apply(get_vital_name)\n",
    "\n",
    "# Aggregate statistics per ID and Vital\n",
    "df_stats = df_filtered.groupby([\"ID\", \"VITAL_NAME\"])[\"ENTERED_VALUE\"].agg([\"min\", \"max\", \"mean\"]).reset_index()\n",
    "\n",
    "# Pivot table to create separate columns for each vital and statistic\n",
    "df_pivot = df_stats.pivot(index=\"ID\", columns=\"VITAL_NAME\", values=[\"min\", \"max\", \"mean\"])\n",
    "\n",
    "# Rename columns for clarity (e.g., min_HEIGHT, max_HEIGHT, mean_HEIGHT)\n",
    "df_pivot.columns = [f\"{stat}_{vital}\" for stat, vital in df_pivot.columns]\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Ensure all vital-related variables exist in df_encoded, even if missing in df_pivot\n",
    "for vital in vitals:\n",
    "    for stat in [\"min\", \"max\", \"mean\"]:\n",
    "        col_name = f\"{stat}_{vital}\"\n",
    "        if col_name not in df_pivot.columns:\n",
    "            df_pivot[col_name] = 0  # Fill missing columns with 0\n",
    "\n",
    "# Merge with df_encoded\n",
    "df_encoded = df_encoded.merge(df_pivot, on=\"ID\", how=\"left\")\n",
    "\n",
    "# Replace NaN values in the new columns with 0\n",
    "for vital in vitals:\n",
    "    for stat in [\"min\", \"max\", \"mean\"]:\n",
    "        col_name = f\"{stat}_{vital}\"\n",
    "        df_encoded[col_name] = df_encoded[col_name].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the anonymized blood pressure data\n",
    "bp_df = pd.read_csv(\"data/anonymized_H43413_bp.csv\")\n",
    "\n",
    "# Ensure ID columns are of the same type\n",
    "bp_df['ID'] = bp_df['ID'].astype(str)\n",
    "df_encoded['ID'] = df_encoded['ID'].astype(str)\n",
    "\n",
    "# Compute blood pressure statistics for each patient\n",
    "bp_stats = bp_df.groupby('ID').agg({\n",
    "    'SYSTOLIC_BP': ['min', 'max', 'mean'],\n",
    "    'DIASTOLIC_BP': ['min', 'max', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "bp_stats.columns = ['ID', 'SYSTOLIC_BP_min', 'SYSTOLIC_BP_max', 'SYSTOLIC_BP_mean',\n",
    "                    'DIASTOLIC_BP_min', 'DIASTOLIC_BP_max', 'DIASTOLIC_BP_mean']\n",
    "\n",
    "# Merge with df_encoded\n",
    "df_encoded = df_encoded.merge(bp_stats, on='ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the lab test data\n",
    "lab_data_path = \"data/anonymized_h43413_labs.csv\"\n",
    "lab_data = pd.read_csv(lab_data_path)\n",
    "\n",
    "# Define the tests of interest\n",
    "lab_tests = [\"CREATININE\", \"BMC_GLUCOSE\", \"BMC_ALT(SGPT)\", \"BMC_AST(SGOT)\"]\n",
    "\n",
    "# Filter only rows that match these lab tests\n",
    "filtered_labs = lab_data[lab_data[\"RESULT_TEST_NM\"].isin(lab_tests)]\n",
    "\n",
    "# Ensure numeric values for results\n",
    "filtered_labs[\"RESULT_VALUE_NUM\"] = pd.to_numeric(filtered_labs[\"RESULT_VALUE_NUM\"], errors=\"coerce\")\n",
    "\n",
    "# Load the encoded patient list\n",
    "patient_ids = df_encoded[\"ID\"].unique()\n",
    "\n",
    "# Filter lab data to only include IDs present in df_encoded\n",
    "filtered_labs = filtered_labs[filtered_labs[\"ID\"].isin(patient_ids)]\n",
    "\n",
    "# Compute min, max, mean for each lab test per patient\n",
    "lab_summary = (\n",
    "    filtered_labs.groupby([\"ID\", \"RESULT_TEST_NM\"])[\"RESULT_VALUE_NUM\"]\n",
    "    .agg([\"min\", \"max\", \"mean\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Reshape data to have separate columns for each test\n",
    "lab_summary = lab_summary.pivot(index=\"ID\", columns=\"RESULT_TEST_NM\", values=[\"min\", \"max\", \"mean\"])\n",
    "\n",
    "# Flatten multi-index column names\n",
    "lab_summary.columns = [f\"{stat}_{test}\" for stat, test in lab_summary.columns]\n",
    "lab_summary.reset_index(inplace=True)\n",
    "\n",
    "# Merge the computed lab stats back into df_encoded\n",
    "df_encoded = df_encoded.merge(lab_summary, on=\"ID\", how=\"left\")\n",
    "\n",
    "\n",
    "# Load the Problem List sheet from the Excel file\n",
    "problem_list_df = pd.read_excel(\"data/anonymized_H-43413 Data Add On.xlsx\", sheet_name=\"Problem List\")\n",
    "\n",
    "# Ensure ID columns are of the same type\n",
    "problem_list_df['ID'] = problem_list_df['ID'].astype(str)\n",
    "df_encoded['ID'] = df_encoded['ID'].astype(str)\n",
    "\n",
    "# List of ICD-10 codes to check\n",
    "icd10_codes = [\n",
    "    \"F17.200\", \"F20.9\", \"F39\", \"F25.9\", \"F43.10\", \"F29\", \"F32.A\", \"F41.1\", \"F41.9\", \"F11.20\", \"F31.9\", \"F22\", \n",
    "    \"F25.0\", \"F20.0\", \"F33.1\", \"F19.10\", \"F10.20\", \"F10.10\", \"F43.20\", \"F14.10\", \"F32.9\", \"F17.210\", \"F14.20\", \n",
    "    \"F99\", \"F25.1\", \"F12.10\", \"F19.90\", \"F90.9\", \"F33.9\", \"F11.21\", \"F33.3\", \"F10.21\", \"F11.90\", \"F12.20\", \n",
    "    \"F43.21\", \"F19.20\", \"F20.3\", \"F10.11\", \"F41.8\", \"F41.0\", \"F33.2\", \"F20.89\", \"F34.1\"\n",
    "]\n",
    "\n",
    "# Initialize binary columns in df_encoded\n",
    "df_encoded[icd10_codes] = 0\n",
    "\n",
    "# Map ICD-10 codes to binary variables\n",
    "for index, row in problem_list_df.iterrows():\n",
    "    patient_id = row['ID']\n",
    "    icd_list = str(row['CURRENT ICD-10 LIST']).split(', ')\n",
    "    \n",
    "    if patient_id in df_encoded['ID'].values:\n",
    "        for icd_code in icd10_codes:\n",
    "            if icd_code in icd_list:\n",
    "                df_encoded.loc[df_encoded['ID'] == patient_id, icd_code] = 1\n",
    "\n",
    "def update_primary_race(df):\n",
    "    # Set 'PRIMARY_RACE_Asian' to 1 where 'PRIMARY_RACE_Chinese' is 1\n",
    "    df.loc[df['PRIMARY_RACE_Chinese'] == 1, 'PRIMARY_RACE_Asian'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_Native Hawaiian / Pacific Islander' to 1 if certain conditions are met\n",
    "    df.loc[df['PRIMARY_RACE_Native Hawaiian / Other Pacific Islander'] == 1, 'PRIMARY_RACE_Native Hawaiian / Pacific Islander'] = 1\n",
    "    df.loc[df['PRIMARY_RACE_Other Pacific Islander'] == 1, 'PRIMARY_RACE_Native Hawaiian / Pacific Islander'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_American Indian / Native American' to 1 if 'PRIMARY_RACE_American Indian or Alaskan Native' is 1\n",
    "    df.loc[df['PRIMARY_RACE_American Indian or Alaskan Native'] == 1, 'PRIMARY_RACE_American Indian / Native American'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_Middle Eastern' to 1 if 'PRIMARY_ETHNICITY_Middle Eastern' is 1\n",
    "    df.loc[df['PRIMARY_ETHNICITY_Middle Eastern'] == 1, 'PRIMARY_RACE_Middle Eastern'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_White' to 1 if any of the specified ethnicity columns are 1\n",
    "    df.loc[df[['PRIMARY_ETHNICITY_Russian', 'PRIMARY_ETHNICITY_American', 'PRIMARY_ETHNICITY_European']].sum(axis=1) > 0, 'PRIMARY_RACE_White'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_Asian Indian' to 1 if 'PRIMARY_ETHNICITY_Asian Indian' is 1\n",
    "    df.loc[df['PRIMARY_ETHNICITY_Asian Indian'] == 1, 'PRIMARY_RACE_Asian Indian'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_Black / African American' to 1 if any of the specified ethnicity columns are 1\n",
    "    df.loc[df[['PRIMARY_ETHNICITY_African', 'PRIMARY_ETHNICITY_African American', 'PRIMARY_ETHNICITY_Cape Verdean', 'PRIMARY_ETHNICITY_Caribbean Islander', 'PRIMARY_ETHNICITY_Haitian', 'PRIMARY_ETHNICITY_Middle Eastern or North African']].sum(axis=1) > 0, 'PRIMARY_RACE_Black / African American'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_Asian' to 1 if any of the specified ethnicity columns are 1\n",
    "    df.loc[df[['PRIMARY_ETHNICITY_Cambodian', 'PRIMARY_ETHNICITY_Chinese', 'PRIMARY_ETHNICITY_Filipino', 'PRIMARY_ETHNICITY_Japanese', 'PRIMARY_ETHNICITY_Korean', 'PRIMARY_ETHNICITY_Laotian', 'PRIMARY_ETHNICITY_Vietnamese']].sum(axis=1) > 0, 'PRIMARY_RACE_Asian'] = 1\n",
    "    \n",
    "    # Set 'PRIMARY_RACE_Unknown' based on the presence of any PRIMARY_RACE_ variables\n",
    "    race_columns = [col for col in df.columns if col.startswith('PRIMARY_RACE_') and col != 'PRIMARY_RACE_Unknown']\n",
    "    df['PRIMARY_RACE_Unknown'] = (df[race_columns].sum(axis=1) == 0).astype(int)\n",
    "    \n",
    "    # Set 'PRIMARY_ETHNICITY_Unknown' based on the presence of any PRIMARY_ETHNICITY_ variables\n",
    "    ethnicity_columns = [col for col in df.columns if col.startswith('PRIMARY_ETHNICITY_') and col != 'PRIMARY_ETHNICITY_Unknown']\n",
    "    df['PRIMARY_ETHNICITY_Unknown'] = (df[ethnicity_columns].sum(axis=1) == 0).astype(int)\n",
    "    \n",
    "    # Drop the specified columns\n",
    "    df.drop(columns=['PRIMARY_RACE_Chinese', 'PRIMARY_RACE_Other Pacific Islander', 'PRIMARY_RACE_Native Hawaiian / Other Pacific Islander', 'PRIMARY_RACE_American Indian or Alaskan Native'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to df_encoded\n",
    "df_encoded = update_primary_race(df_encoded)\n",
    "\n",
    "\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khang\\AppData\\Local\\Temp\\ipykernel_15100\\3294417873.py:47: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_encoded = df_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file_paths = [\n",
    "    \"data/anonymized_H-43413 Amb Med Data.xlsx\",\n",
    "    \"data/anonymized_H43413_vitals.csv\",\n",
    "    \"data/anonymized_H-43413 Qualifying Encounters.xlsx\",\n",
    "    \"data/anonymized_h43413_labs.csv\",\n",
    "    \"data/anonymized_H-43413 Data Add On.xlsx\",\n",
    "    \"data/anonymized_H43413_bp.csv\"\n",
    "]\n",
    "\n",
    "# Load df_encoded (Assuming df_encoded is already loaded)\n",
    "df_encoded_ids = set(df_encoded['ID'].astype(str))  # Convert to string for consistency\n",
    "\n",
    "# Function to get valid IDs from a spreadsheet\n",
    "def get_valid_ids(file_path):\n",
    "    try:\n",
    "        \n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path, usecols=['ID'])  # Load only ID column\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path, usecols=['ID'])  # Load only ID column\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            \n",
    "            return set()\n",
    "        # Load spreadsheet (CSV or Excel)\n",
    "        if file_path ==  \"data/anonymized_H-43413 Data Add On.xlsx\":\n",
    "            df = pd.read_excel(\"data/anonymized_H-43413 Data Add On.xlsx\", sheet_name=\"Problem List\", usecols=['ID'])\n",
    "        \n",
    "\n",
    "        return set(df['ID'].dropna().astype(str))  # Convert to string and remove NaNs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "# Find IDs that exist in all spreadsheets\n",
    "valid_ids = df_encoded_ids  # Start with df_encoded IDs\n",
    "for file in file_paths:\n",
    "    valid_ids &= get_valid_ids(file)  # Keep only IDs present in all files\n",
    "\n",
    "# Filter df_encoded to keep only rows where ID is in valid_ids\n",
    "df_encoded = df_encoded[df_encoded['ID'].astype(str).isin(valid_ids)]\n",
    "\n",
    "# Convert boolean values to integers (0 and 1)\n",
    "df_encoded = df_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_pickle(\"df_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'demo_age', 'RPL_THEME1', 'GENDER_F', 'GENDER_M', 'GENDER_U', 'PRIMARY_RACE_American Indian / Native American', 'PRIMARY_RACE_Asian', 'PRIMARY_RACE_Asian Indian', 'PRIMARY_RACE_Black / African American', 'PRIMARY_RACE_Hispanic or Latino', 'PRIMARY_RACE_Middle Eastern', 'PRIMARY_RACE_Native Hawaiian / Pacific Islander', 'PRIMARY_RACE_Other', 'PRIMARY_RACE_Unknown', 'PRIMARY_RACE_White', 'LANGUAGE_Afrikaans', 'LANGUAGE_Albanian', 'LANGUAGE_American Sign Language', 'LANGUAGE_American Sign language & Certified Deaf Interpreter', 'LANGUAGE_Amharic / Ethiopia', 'LANGUAGE_Arabic', 'LANGUAGE_Bassa / Liberia', 'LANGUAGE_Bengali / Hindi / Urdu', 'LANGUAGE_Bosnian / Croatian / Yulo', 'LANGUAGE_Brazilian Portuguese', 'LANGUAGE_Cape Verdean / Port Creole', 'LANGUAGE_Chinese / Cantonese', 'LANGUAGE_Chinese / Mandarin', 'LANGUAGE_English', 'LANGUAGE_French', 'LANGUAGE_Fulani / Cameroon', 'LANGUAGE_German', 'LANGUAGE_Haitian Creole', 'LANGUAGE_Italian', 'LANGUAGE_Kirundi / Burundi', 'LANGUAGE_Korean', 'LANGUAGE_Luganda / Uganda', 'LANGUAGE_Nepali', 'LANGUAGE_Oromo / Ethiopia', 'LANGUAGE_Other', 'LANGUAGE_Polish', 'LANGUAGE_Portuguese', 'LANGUAGE_Romanian', 'LANGUAGE_Russian', 'LANGUAGE_Somali', 'LANGUAGE_Spanish', 'LANGUAGE_Tagalog', 'LANGUAGE_Tigrinya', 'LANGUAGE_Turkish', 'LANGUAGE_Ukrainian', 'LANGUAGE_Unknown', 'LANGUAGE_Vietnamese', 'PRIMARY_ETHNICITY_African', 'PRIMARY_ETHNICITY_African American', 'PRIMARY_ETHNICITY_American', 'PRIMARY_ETHNICITY_Asian Indian', 'PRIMARY_ETHNICITY_Brazilian', 'PRIMARY_ETHNICITY_Cambodian', 'PRIMARY_ETHNICITY_Cape Verdean', 'PRIMARY_ETHNICITY_Caribbean Islander', 'PRIMARY_ETHNICITY_Chinese', 'PRIMARY_ETHNICITY_Colombian', 'PRIMARY_ETHNICITY_Cuban', 'PRIMARY_ETHNICITY_Dominican', 'PRIMARY_ETHNICITY_European', 'PRIMARY_ETHNICITY_Filipino', 'PRIMARY_ETHNICITY_Guatemalan', 'PRIMARY_ETHNICITY_Haitian', 'PRIMARY_ETHNICITY_Honduran', 'PRIMARY_ETHNICITY_Japanese', 'PRIMARY_ETHNICITY_Korean', 'PRIMARY_ETHNICITY_Laotian', 'PRIMARY_ETHNICITY_Mexican, Mexican American, Chicano', 'PRIMARY_ETHNICITY_Mexican, Mexican American, Chicano/a', 'PRIMARY_ETHNICITY_Middle Eastern', 'PRIMARY_ETHNICITY_Middle Eastern or North African', 'PRIMARY_ETHNICITY_Other', 'PRIMARY_ETHNICITY_Portuguese', 'PRIMARY_ETHNICITY_Puerto Rican', 'PRIMARY_ETHNICITY_Russian', 'PRIMARY_ETHNICITY_Salvadoran', 'PRIMARY_ETHNICITY_Unknown', 'PRIMARY_ETHNICITY_Vietnamese', 'D_Insur_at_pull_Other', 'D_Insur_at_pull_Other Government', 'D_Insur_at_pull_Private', 'D_Insur_at_pull_Public', 'D_Insur_at_pull_Unknown', 'F28', 'F20.5', 'F20.2', 'F21', 'F23', 'F24', 'F20.81', 'F25.8', 'F20.1', 'min_CLOZAPINE', 'min_OLANZAPINE', 'min_RISPERIDONE', 'max_CLOZAPINE', 'max_OLANZAPINE', 'max_RISPERIDONE', 'mean_CLOZAPINE', 'mean_OLANZAPINE', 'mean_RISPERIDONE', 'min_BMI', 'min_HEIGHT', 'min_PULSE', 'min_WEIGHT', 'max_BMI', 'max_HEIGHT', 'max_PULSE', 'max_WEIGHT', 'mean_BMI', 'mean_HEIGHT', 'mean_PULSE', 'mean_WEIGHT', 'SYSTOLIC_BP_min', 'SYSTOLIC_BP_max', 'SYSTOLIC_BP_mean', 'DIASTOLIC_BP_min', 'DIASTOLIC_BP_max', 'DIASTOLIC_BP_mean', 'F17.200', 'F20.9', 'F39', 'F25.9', 'F43.10', 'F29', 'F32.A', 'F41.1', 'F41.9', 'F11.20', 'F31.9', 'F22', 'F25.0', 'F20.0', 'F33.1', 'F19.10', 'F10.20', 'F10.10', 'F43.20', 'F14.10', 'F32.9', 'F17.210', 'F14.20', 'F99', 'F25.1', 'F12.10', 'F19.90', 'F90.9', 'F33.9', 'F11.21', 'F33.3', 'F10.21', 'F11.90', 'F12.20', 'F43.21', 'F19.20', 'F20.3', 'F10.11', 'F41.8', 'F41.0', 'F33.2', 'F20.89', 'F34.1']\n"
     ]
    }
   ],
   "source": [
    "print(df_encoded.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
